{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#!/usr/bin/env python"]}, {"cell_type": "markdown", "metadata": {}, "source": ["*********************************************************************<br>\n", "MAIN PROGRAM TO COMPUTE A DESIGN MATRIX TO INVERT FOR STRUCTURE --<br>\n", "<br>\n", "Copyright (c) 2022-2024: HILARY R. MARTENS<br>\n", "<br>\n", "This file is part of LoadDef.<br>\n", "<br>\n", "   LoadDef is free software: you can redistribute it and/or modify<br>\n", "   it under the terms of the GNU General Public License as published by<br>\n", "   the Free Software Foundation, either version 3 of the License, or<br>\n", "   any later version.<br>\n", "<br>\n", "   LoadDef is distributed in the hope that it will be useful,<br>\n", "   but WITHOUT ANY WARRANTY; without even the implied warranty of<br>\n", "   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the<br>\n", "   GNU General Public License for more details.<br>\n", "<br>\n", "   You should have received a copy of the GNU General Public License<br>\n", "   along with LoadDef.  If not, see <https://www.gnu.org/licenses/>.<br>\n", "<br>\n", "*********************************************************************"]}, {"cell_type": "markdown", "metadata": {}, "source": ["IMPORT PRINT FUNCTION"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from __future__ import print_function"]}, {"cell_type": "markdown", "metadata": {}, "source": ["IMPORT MPI MODULE"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from mpi4py import MPI"]}, {"cell_type": "markdown", "metadata": {}, "source": ["MODIFY PYTHON PATH TO INCLUDE 'LoadDef' DIRECTORY"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import sys\n", "import os\n", "sys.path.append(os.getcwd() + \"/../\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["IMPORT PYTHON MODULES"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import scipy as sc\n", "import datetime\n", "import netCDF4 \n", "from math import pi\n", "from scipy import interpolate\n", "from LOADGF.utility import perturb_pmod\n", "from LOADGF.LN import compute_love_numbers\n", "from LOADGF.GF import compute_greens_functions\n", "from CONVGF.CN import load_convolution\n", "from CONVGF.utility import read_station_file\n", "from CONVGF.utility import read_lsmask\n", "from CONVGF.utility import read_greens_fcn_file\n", "from CONVGF.utility import read_greens_fcn_file_norm\n", "from CONVGF.utility import normalize_greens_fcns\n", "from CONVGF.utility import read_AmpPha\n", "from CONVGF.CN import load_convolution\n", "from CONVGF.CN import interpolate_load\n", "from CONVGF.CN import compute_specific_greens_fcns\n", "from CONVGF.CN import generate_integration_mesh\n", "from CONVGF.CN import intmesh2geogcoords\n", "from CONVGF.CN import integrate_greens_fcns\n", "from CONVGF.CN import compute_angularDist_azimuth\n", "from CONVGF.CN import interpolate_lsmask\n", "from CONVGF.CN import coef2amppha\n", "from CONVGF.CN import mass_conservation\n", "from utility.pmes import combine_stations\n", "from CONVGF.utility import read_convolution_file"]}, {"cell_type": "markdown", "metadata": {}, "source": ["--------------- SPECIFY USER INPUTS --------------------- #"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Full path to planet model text file<br>\n", "    Planet model should be spherically symmetric, elastic,<br>\n", "        non-rotating, and isotropic (SNREI)<br>\n", "    Format: radius(km), vp(km/s), vs(km/s), density(g/cc)<br>\n", "    If the file delimiter is not whitespace, then specify in<br>\n", "        call to function."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["pmod = \"PREM\"\n", "planet_model = (\"../input/Planet_Models/\" + pmod + \".txt\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Perturbation used for the forward-model runs"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["perturbation = np.log10(1.01)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Regions perturbed in the forward-model runs<br>\n", "  Note: The second-order Tikhonov regularization in StructSolv will only work properly if the layers stack on one another.<br>\n", "        For example, the bottom radius of the top-most layer is the top radius of the next layer down. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["nodes = [[6351.,6371.],[6331.,6351.],[6311.,6331.]]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Reference frame [Blewitt 2003]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["rfm = \"cm\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Full Path to Load Directory and Prefix of Filename"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["loadfile_directory = (\"../output/Grid_Files/nc/OTL/\") "]}, {"cell_type": "markdown", "metadata": {}, "source": ["Prefix for the Load Files (Load Directory will be Searched for all Files Starting with this Prefix)<br>\n", " :: Note: For Load Files Organized by Date, the End of Filename Name Must be in the Format yyyymmddhhmnsc.txt<br>\n", " :: Note: If not organized by date, files may be organized by tidal harmonic, for example (i.e. a unique filename ending)<br>\n", " :: Note: Output names (within output files) will be determined by extension following last underscore character (e.g., date/harmonic/model)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["loadfile_prefix = (\"convgf_GOT410c\") "]}, {"cell_type": "markdown", "metadata": {}, "source": ["LoadFile Format: [\"nc\", \"txt\"]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["loadfile_format = \"nc\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Include imaginary component? For harmonic loads, such as tides, set to \"True.\" Otherwise, for standard displacement data, set to \"False.\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["inc_imag = True"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Are the Load Files Organized by Datetime?<br>\n", " :: If False, all Files that match the loadfile directory and prefix will be analyzed."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["time_series = False  "]}, {"cell_type": "markdown", "metadata": {}, "source": ["Date Range for Computation (Year,Month,Day,Hour,Minute,Second)<br>\n", " :: Note: Only used if 'time_series' is True"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["frst_date = [2015,1,1,0,0,0]\n", "last_date = [2016,3,1,0,0,0]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Are the load values on regular grids (speeds up interpolation); If unsure, leave as false."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["regular = True"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Load Density<br>\n", " Recommended: 1025-1035 kg/m^3 for oceanic loads (e.g., FES2014, ECCO2); 1 kg/m^3 for atmospheric loads (e.g. ECMWF); 1000 kg/m^3 for fresh water"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ldens = 1030.0"]}, {"cell_type": "markdown", "metadata": {}, "source": ["NEW OPTION: Provide a common geographic mesh?<br>\n", "If True, must provide the full path to a mesh file (see: GRDGEN/common_mesh). <br>\n", "If False, a station-centered grid will be created within the functions called here. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["common_mesh = True\n", "# Full Path to Grid File Containing Surface Mesh (for sampling the load Green's functions)\n", "#  :: Format: latitude midpoints [float,degrees N], longitude midpoints [float,degrees E], unit area of each patch [float,dimensionless (need to multiply by r^2)]\n", "meshfname = (\"commonMesh_global_1.0_1.0_18.0_60.0_213.0_278.0_0.1_0.1_28.0_50.0_233.0_258.0_0.01_0.01_landmask\")\n", "convmesh = (\"../output/Grid_Files/nc/commonMesh/\" + meshfname + \".nc\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Planet Radius (in meters; used for Greens function normalization)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["planet_radius = 6371000.\n", "  \n", "# Ocean/Land Mask \n", "#  :: 0 = do not mask ocean or land (retain full model); 1 = mask out land (retain ocean); 2 = mask out oceans (retain land)\n", "#  :: Recommended: 1 for oceanic; 2 for atmospheric\n", "lsmask_type = 1"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Full Path to Land-Sea Mask File (May be Irregular and Sparse)<br>\n", " :: Format: Lat, Lon, Mask [0=ocean; 1=land]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["lsmask_file = (\"../input/Land_Sea/ETOPO1_Ice_g_gmt4_wADD.txt\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Enforce mass conservation by removing a spatial mean from the load grid?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["mass_cons = False"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Station/Grid-Point Location File (Lat, Lon, StationName)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sta_file = (\"../input/Station_Locations/NOTA.txt\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Overwrite old convolution files? (May be helpful if you want to change / add load files, but keep everything else the same)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["overwrite = True"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Optional: Additional string to include in all output filenames (Love numbers, Green's functions, Convolution)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["outstr = (\"\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Optional: Additional string to include in output filenames for the convolution (e.g. \"_2022\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if (common_mesh == True):\n", "    mtag = \"commonMesh\"\n", "else:\n", "    mtag = \"stationMesh\"\n", "outstr_conv = (\"_dens\" + str(int(ldens)) + \"_\" + mtag)\n", " \n", "# ------------------ END USER INPUTS ----------------------- #"]}, {"cell_type": "markdown", "metadata": {}, "source": ["-------------------- SETUP MPI --------------------------- #"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Get the Main MPI Communicator That Controls Communication Between Processors"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["comm = MPI.COMM_WORLD\n", "# Get My \"Rank\", i.e. the Processor Number Assigned to Me\n", "rank = comm.Get_rank()\n", "# Get the Total Number of Other Processors Used\n", "size = comm.Get_size()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---------------------------------------------------------- #"]}, {"cell_type": "markdown", "metadata": {}, "source": ["-------------------- BEGIN CODE -------------------------- #"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Ensure that the Output Directories Exist"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if (rank == 0):\n", "    if not (os.path.isdir(\"../output/Convolution/\")):\n", "        os.makedirs(\"../output/Convolution/\")\n", "    if not (os.path.isdir(\"../output/DesignMatrixStructure/\")):\n", "        os.makedirs(\"../output/DesignMatrixStructure/\")\n", "    if not (os.path.isdir(\"../output/CombineStations/\")):\n", "        os.makedirs(\"../output/CombineStations/\")\n", "    if not (os.path.isdir(\"../output/Love_Numbers/\")):\n", "        os.makedirs(\"../output/Love_Numbers/\")\n", "    if not (os.path.isdir(\"../output/Love_Numbers/LLN/\")):\n", "        os.makedirs(\"../output/Love_Numbers/LLN\")\n", "    if not (os.path.isdir(\"../output/Love_Numbers/PLN/\")):\n", "        os.makedirs(\"../output/Love_Numbers/PLN\")\n", "    if not (os.path.isdir(\"../output/Love_Numbers/STR/\")):\n", "        os.makedirs(\"../output/Love_Numbers/STR\")\n", "    if not (os.path.isdir(\"../output/Love_Numbers/SHR/\")):\n", "        os.makedirs(\"../output/Love_Numbers/SHR\")\n", "    if not (os.path.isdir(\"../output/Greens_Functions/\")):\n", "        os.makedirs(\"../output/Greens_Functions/\")\n", "    if not (os.path.isdir(\"../output/Planet_Models/\")):\n", "        os.makedirs(\"../output/Planet_Models/\")\n", "    if not (os.path.isdir(\"../output/Convolution/temp/\")):\n", "        os.makedirs(\"../output/Convolution/temp/\")\n", "tempdir = \"../output/Convolution/temp/\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Check format of load files"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if not (loadfile_format == \"nc\"):\n", "    if not (loadfile_format == \"txt\"):\n", "        print(\":: Error: Invalid format for load files. See scripts in the /GRDGEN/load_files/ folder. Acceptable formats: netCDF, txt.\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Make sure all jobs have finished before continuing"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["comm.Barrier()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---------------- BEGIN PERTURB MODEL ---------------------- #"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Create a list of planetary models based on layers to perturb"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["pmodels = [] # Names of planetary models\n", "lngfext = [] # Extensions for Love number and Green's function files\n", "lnfiles = [] # Names of load Love number files\n", "gffiles = [] # Names of load Green's function files\n", "outdir_pmods = (\"../output/Planet_Models/\")\n", "# Loop through nodes\n", "for ee in range(0,len(nodes)):\n", "    # Current radial range\n", "    crad_range = nodes[ee]\n", "    # RUN THE PERTURBATIONS\n", "    perturb_pmod.main(planet_model,pmod,perturbation,crad_range,outdir_pmods,suffix=outstr)\n", "    # Current output name (must match what \"perturb_pmod\" produces!)\n", "    outname = (str('{:.4f}'.format(perturbation)) + \"_\" + str(crad_range[0]) + \"_\" + str(crad_range[1]) + outstr)\n", "    # New model for mu\n", "    mu_name = (pmod + \"_mu_\" + outname)\n", "    fname_mu = (outdir_pmods + mu_name + \".txt\")\n", "    lngfext_mu = (mu_name + outstr + \".txt\")\n", "    ln_mu = (\"../output/Love_Numbers/LLN/lln_\" + lngfext_mu)\n", "    gf_mu = (\"../output/Greens_Functions/\" + rfm + \"_\" + lngfext_mu)\n", "    # New model for kappa\n", "    kappa_name = (pmod + \"_kappa_\" + outname)\n", "    fname_kappa = (outdir_pmods + kappa_name + \".txt\")\n", "    lngfext_kappa = (kappa_name + outstr + \".txt\")\n", "    ln_kappa = (\"../output/Love_Numbers/LLN/lln_\" + lngfext_kappa)\n", "    gf_kappa = (\"../output/Greens_Functions/\" + rfm + \"_\" + lngfext_kappa)\n", "    # New model for rho\n", "    rho_name = (pmod + \"_rho_\" + outname)\n", "    fname_rho = (outdir_pmods + rho_name + \".txt\")\n", "    lngfext_rho = (rho_name + outstr + \".txt\")\n", "    ln_rho = (\"../output/Love_Numbers/LLN/lln_\" + lngfext_rho)\n", "    gf_rho = (\"../output/Greens_Functions/\" + rfm + \"_\" + lngfext_rho)\n", "    # Append files to list\n", "    pmodels.append(fname_mu)\n", "    pmodels.append(fname_kappa)\n", "    pmodels.append(fname_rho)\n", "    lngfext.append(lngfext_mu)\n", "    lngfext.append(lngfext_kappa)\n", "    lngfext.append(lngfext_rho)\n", "    lnfiles.append(ln_mu)\n", "    lnfiles.append(ln_kappa)\n", "    lnfiles.append(ln_rho)\n", "    gffiles.append(gf_mu)\n", "    gffiles.append(gf_kappa)\n", "    gffiles.append(gf_rho)\n", "# Append original model\n", "pmodels.append(planet_model)\n", "lngfext.append(pmod + outstr + \".txt\")\n", "lnfiles.append(\"../output/Love_Numbers/LLN/lln_\" + pmod + outstr + \".txt\")\n", "gffiles.append(\"../output/Greens_Functions/\" + rfm + \"_\" + pmod + outstr + \".txt\")    "]}, {"cell_type": "markdown", "metadata": {}, "source": ["---------------- END PERTURB MODEL ----------------------- #"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [" \n", "# ---------------- BEGIN LOVE NUMBERS ---------------------- #"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Loop through planetary models "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for bb in range(0,len(pmodels)): \n\n", "    # Current model\n", "    cpmod = pmodels[bb]\n\n", "    # Output filename\n", "    file_ext = lngfext[bb]\n\n", "    # Check if file already exists\n", "    if (os.path.isfile(lnfiles[bb])):\n", "        continue\n", "    else: \n", " \n", "        # Compute the Love numbers (Load and Potential)\n", "        if (rank == 0):\n", "            # Compute Love Numbers\n", "            ln_n,ln_h,ln_nl,ln_nk,ln_h_inf,ln_l_inf,ln_k_inf,ln_h_inf_p,ln_l_inf_p,ln_k_inf_p,\\\n", "                ln_hpot,ln_nlpot,ln_nkpot,ln_hstr,ln_nlstr,ln_nkstr,ln_hshr,ln_nlshr,ln_nkshr,\\\n", "                ln_planet_radius,ln_planet_mass,ln_sint,ln_Yload,ln_Ypot,ln_Ystr,ln_Yshr,\\\n", "                ln_lmda_surface,ln_mu_surface = \\\n", "                compute_love_numbers.main(cpmod,rank,comm,size,file_out=file_ext)\n", "        # For Worker Ranks, Run the Code But Don't Return Any Variables\n", "        else:\n", "            # Workers Compute Love Numbers\n", "            compute_love_numbers.main(cpmod,rank,comm,size,file_out=file_ext)\n", "            # Workers Will Know Nothing About the Data Used to Compute the GFs\n", "            ln_n = ln_h = ln_nl = ln_nk = ln_h_inf = ln_l_inf = ln_k_inf = ln_h_inf_p = ln_l_inf_p = ln_k_inf_p = None\n", "            ln_planet_radius = ln_planet_mass = ln_Yload = ln_Ypot = ln_Ystr = ln_Yshr = None\n", "            ln_hpot = ln_nlpot = ln_nkpot = ln_hstr = ln_nlstr = ln_nkstr = ln_hshr = None\n", "            ln_nlshr = ln_nkshr = ln_sint = ln_lmda_surface = ln_mu_surface = None"]}, {"cell_type": "markdown", "metadata": {}, "source": ["----------------- END LOVE NUMBERS ----------------------- #"]}, {"cell_type": "markdown", "metadata": {}, "source": ["-------------- BEGIN GREENS FUNCTIONS -------------------- #"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Make sure all jobs have finished before continuing"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["comm.Barrier()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Set normalization flag"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["norm_flag  = False"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Loop through Love number files"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for cc in range(0,len(lnfiles)):\n\n", "    # Current Love number file\n", "    lln_file = lnfiles[cc]\n\n", "    # Output filename\n", "    file_out = lngfext[cc]\n\n", "    # Check if file already exists\n", "    if (os.path.isfile(gffiles[cc])):\n", "        continue\n", "    else: \n", "  \n", "        # Compute the Displacement Greens functions (For Load Love Numbers Only)\n", "        if (rank == 0):\n", "            u,v,u_norm,v_norm,u_cm,v_cm,u_norm_cm,v_norm_cm,u_cf,v_cf,u_norm_cf,v_norm_cf,gE,gE_norm,gE_cm,gE_cm_norm,\\\n", "                gE_cf,gE_cf_norm,tE,tE_norm,tE_cm,tE_cm_norm,tE_cf,tE_cf_norm,\\\n", "                e_tt,e_ll,e_rr,e_tt_norm,e_ll_norm,e_rr_norm,e_tt_cm,e_ll_cm,e_rr_cm,e_tt_cm_norm,e_ll_cm_norm,e_rr_cm_norm,\\\n", "                e_tt_cf,e_ll_cf,e_rr_cf,e_tt_cf_norm,e_ll_cf_norm,e_rr_cf_norm,gN,tN = \\\n", "                    compute_greens_functions.main(lln_file,rank,comm,size,grn_out=file_out)\n", "        # For Worker Ranks, Run the Code But Don't Return Any Variables\n", "        else:\n", "            compute_greens_functions.main(lln_file,rank,comm,size,grn_out=file_out)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["-------------- END GREENS FUNCTIONS ---------------------- #"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---------------- BEGIN CONVOLUTIONS ---------------------- #"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Ensure that the Output Directories Exist & Read in the Stations"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if (rank == 0):\n\n", "    # Read Station File\n", "    slat,slon,sta = read_station_file.main(sta_file)\n\n", "    # Ensure that Station Locations are in Range 0-360\n", "    neglon_idx = np.where(slon<0.)\n", "    slon[neglon_idx] += 360.\n\n", "    # Determine Number of Stations Read In\n", "    if isinstance(slat,float) == True: # only 1 station\n", "        numel = 1\n", "    else:\n", "        numel = len(slat)\n\n", "    # Generate an Array of File Indices\n", "    sta_idx = np.linspace(0,numel,num=numel,endpoint=False)\n", "    np.random.shuffle(sta_idx)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["else: # If I'm a worker, I know nothing yet about the data\n", "    slat = slon = sta = numel = sta_idx = None"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Make Sure Everyone Has Reported Back Before Moving On"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["comm.Barrier()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["All Processors Get Certain Arrays and Parameters; Broadcast Them"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sta          = comm.bcast(sta, root=0)\n", "slat         = comm.bcast(slat, root=0)\n", "slon         = comm.bcast(slon, root=0)\n", "numel        = comm.bcast(numel, root=0)\n", "sta_idx      = comm.bcast(sta_idx, root=0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["MPI: Determine the Chunk Sizes for the Convolution"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["total_stations = len(slat)\n", "nominal_load = total_stations // size # Floor Divide\n", "# Final Chunk Might Be Different in Size Than the Nominal Load\n", "if rank == size - 1:\n", "    procN = total_stations - rank * nominal_load\n", "else:\n", "    procN = nominal_load"]}, {"cell_type": "markdown", "metadata": {}, "source": ["File information"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["cndirectory = (\"../output/Convolution/\")\n", "if (lsmask_type == 2):\n", "    cnprefix = (\"cn_LandOnly_\")\n", "elif (lsmask_type == 1):\n", "    cnprefix = (\"cn_OceanOnly_\")\n", "else:\n", "    cnprefix = (\"cn_LandAndOceans_\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Make some preparations that are common to all stations"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if (rank == 0):\n\n", "    # Read in the Land-Sea Mask\n", "    if (lsmask_type > 0):\n", "        lslat,lslon,lsmask = read_lsmask.main(lsmask_file)\n", "    else:\n", "        # Doesn't really matter so long as there are some values filled in with something other than 1 or 2\n", "        lat1d = np.arange(-90.,90.,2.)\n", "        lon1d = np.arange(0.,360.,2.)\n", "        olon,olat = np.meshgrid(lon1d,lat1d)\n", "        lslat = olat.flatten()\n", "        lslon = olon.flatten()\n", "        lsmask = np.ones((len(lslat),)) * -1.\n\n", "    # Ensure that Land-Sea Mask Longitudes are in Range 0-360\n", "    neglon_idx = np.where(lslon<0.)\n", "    lslon[neglon_idx] += 360.\n\n", "    # Convert Start and End Dates to Datetimes\n", "    if (time_series == True):\n", "        frstdt = datetime.datetime(frst_date[0],frst_date[1],frst_date[2],frst_date[3],frst_date[4],frst_date[5])\n", "        lastdt = datetime.datetime(last_date[0],last_date[1],last_date[2],last_date[3],last_date[4],last_date[5])\n\n", "    # Check format of load files\n", "    if not (loadfile_format == \"nc\"):\n", "        if not (loadfile_format == \"txt\"):\n", "            print(\":: Error: Invalid format for load files. See scripts in the /GRDGEN/load_files/ folder. \\\n", "                Acceptable formats: netCDF, txt.\")\n\n", "    # Determine Number of Matching Load Files\n", "    load_files = []\n", "    if os.path.isdir(loadfile_directory):\n", "        for mfile in os.listdir(loadfile_directory): # Filter by Load Directory\n", "            if mfile.startswith(loadfile_prefix): # Filter by File Prefix\n", "                if (time_series == True):\n", "                    if (loadfile_format == \"txt\"):\n", "                        mydt = datetime.datetime.strptime(mfile[-18:-4],'%Y%m%d%H%M%S') # Convert Filename String to Datetime\n", "                    elif (loadfile_format == \"nc\"):\n", "                        mydt = datetime.datetime.strptime(mfile[-17:-3],'%Y%m%d%H%M%S') # Convert Filename String to Datetime\n", "                    else:\n", "                        print(\":: Error: Invalid format for load files. See scripts in the /GRDGEN/load_files/ folder. \\\n", "                            Acceptable formats: netCDF, txt.\")\n", "                    if ((mydt >= frstdt) & (mydt <= lastdt)): # Filter by Date Range\n", "                        load_files.append(loadfile_directory + mfile) # Append File to List\n", "                else:\n", "                    load_files.append(loadfile_directory + mfile) # Append File to List\n", "    else:\n", "        sys.exit('Error: The loadfile directory does not exist. You may need to create it. \\\n", "            The /GRDGEN/load_files/ folder contains utility scripts to convert common models into \\\n", "            LoadDef-compatible formats, and will automatically create a loadfile directory.')\n\n", "    # Test for Load Files\n", "    if not load_files:\n", "        sys.exit('Error: Could not find load files. You may need to generate them. \\\n", "            The /GRDGEN/load_files/ folder contains utility scripts to convert \\\n", "            common models into LoadDef-compatible formats.')\n\n", "    # Sort the Filenames\n", "    load_files = np.asarray(load_files)\n", "    fidx = np.argsort(load_files)\n", "    load_files = load_files[fidx]\n", "    num_lfiles = len(load_files)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["If I'm a Worker, I Know Nothing About the Data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["else:\n", "    lslat = lslon = lsmask = load_files = None\n", "    eamp = epha = namp = npha = vamp = vpha = None"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Make Sure Everyone Has Reported Back Before Moving On"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["comm.Barrier()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Prepare the common mesh, if applicable"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if (rank == 0): \n", "    if (common_mesh == True):\n\n", "        ## Read in the common mesh\n", "        print(':: Common Mesh True. Reading in ilat, ilon, iarea.')\n", "        lcext = convmesh[-2::]\n", "        if (lcext == 'xt'):\n", "            ilat,ilon,unit_area = np.loadtxt(convmesh,usecols=(0,1,2),unpack=True)\n", "            # convert from unit area to true area of the spherical patch in m^2\n", "            iarea = np.multiply(unit_area, planet_radius**2)\n", "        elif (lcext == 'nc'):\n", "            f = netCDF4.Dataset(convmesh)\n", "            ilat = f.variables['midpoint_lat'][:]\n", "            ilon = f.variables['midpoint_lon'][:]\n", "            unit_area = f.variables['unit_area_patch'][:]\n", "            f.close()\n", "            # convert from unit area to true area of the spherical patch in m^2\n", "            iarea = np.multiply(unit_area, planet_radius**2)\n\n", "        ## Determine the Land-Sea Mask: Interpolate onto Mesh\n", "        print(':: Common Mesh True. Applying Land-Sea Mask.')\n", "        print(':: Number of Grid Points: %s | Size of LSMask: %s' %(str(len(ilat)), str(lsmask.shape)))\n", "        lsmk = interpolate_lsmask.main(ilat,ilon,lslat,lslon,lsmask)\n", "        print(':: Finished LSMask Interpolation.')\n\n", "        ## For a common mesh, can already interpolate the load(s) onto the mesh, and also apply the land-sea mask.\n", "        ## Prepare land-sea mask application\n", "        if (lsmask_type == 2):\n", "            test_elements = np.where(lsmk == 0); test_elements = test_elements[0]\n", "        elif (lsmask_type == 1):\n", "            test_elements = np.where(lsmk == 1); test_elements = test_elements[0]\n\n", "        ## Loop through load file(s)\n", "        full_files = []\n", "        for hh in range(0,len(load_files)):\n\n", "            ## Current load file\n", "            cldfile = load_files[hh]\n\n", "            ## Filename identifier\n", "            str_components = cldfile.split('_')\n", "            cext = str_components[-1]\n", "            if (loadfile_format == \"txt\"):\n", "                file_id = cext[0:-4]\n", "            elif (loadfile_format == \"nc\"):\n", "                file_id = cext[0:-3]\n", "            else:\n", "                print(':: Error. Invalid file format for load models. [load_convolution.py]')\n", "                sys.exit()\n\n", "            ## Name of file and check whether it already exists\n", "            custom_file = (tempdir + \"temp\" + outstr_conv + outstr + \"_\" + file_id + \".nc\")\n", "            full_files.append(custom_file)\n", "            if os.path.isfile(custom_file):\n", "                print(':: File exists: ', custom_file, ' -- moving on.')\n", "                continue\n\n", "            ## Read the File\n", "            llat,llon,amp,pha,llat1dseq,llon1dseq,amp2darr,pha2darr = read_AmpPha.main(cldfile,loadfile_format,regular_grid=regular)\n", "            ## Find Where Amplitude is NaN (if anywhere) and Set to Zero\n", "            nanidx = np.isnan(amp); amp[nanidx] = 0.; pha[nanidx] = 0.\n", "            ## Convert Amp/Pha Arrays to Real/Imag\n", "            real = np.multiply(amp,np.cos(np.multiply(pha,pi/180.)))\n", "            imag = np.multiply(amp,np.sin(np.multiply(pha,pi/180.)))\n\n", "            ## Interpolate Load at Each Grid Point onto the Integration Mesh\n", "            ic1,ic2   = interpolate_load.main(ilat,ilon,llat,llon,real,imag,regular)\n\n", "            ## Multiply the Load Heights by the Load Density\n", "            ic1 = np.multiply(ic1,ldens)\n", "            ic2 = np.multiply(ic2,ldens)\n\n", "            ## Enforce Mass Conservation, if Desired\n", "            if (mass_cons == True):\n", "                if (lsmask_type == 1): # For Oceans\n", "                    print(':: Warning: Enforcing Mass Conservation Over Oceans.')\n", "                    ic1_mc,ic2_mc = mass_conservation.main(ic1[lsmk==0],ic2[lsmk==0],iarea[lsmk==0])\n", "                    ic1[lsmk==0] = ic1_mc\n", "                    ic2[lsmk==0] = ic2_mc\n", "                else: # For Land and Whole-Globe Models (like atmosphere and continental water)\n", "                    print(':: Warning: Enforcing Mass Conservation Over Entire Globe.')\n", "                    ic1,ic2 = mass_conservation.main(ic1,ic2,iarea)\n\n", "            ## Apply Land-Sea Mask Based on LS Mask Database (LAND=1;OCEAN=0)\n", "            # If lsmask_type = 2, Set Oceans to Zero (retain land)\n", "            # If lsmask_type = 1, Set Land to Zero (retain ocean)\n", "            # Else, Do Nothing (retain full model)\n", "            if (lsmask_type == 2):\n", "                ic1[lsmk == 0] = 0.\n", "                ic2[lsmk == 0] = 0.\n", "            elif (lsmask_type == 1):\n", "                ic1[lsmk == 1] = 0.\n", "                ic2[lsmk == 1] = 0.\n\n", "            ## Write results to temporary netCDF files\n", "            print(\":: Writing netCDF-formatted temporary file for: \", cldfile)\n", "            # Open new NetCDF file in \"write\" mode\n", "            dataset = netCDF4.Dataset(custom_file,'w',format='NETCDF4_CLASSIC')\n", "            # Define dimensions for variables\n", "            num_pts = len(ic1)\n", "            latitude = dataset.createDimension('latitude',num_pts)\n", "            longitude = dataset.createDimension('longitude',num_pts)\n", "            real = dataset.createDimension('real',num_pts)\n", "            imag = dataset.createDimension('imag',num_pts)\n", "            parea = dataset.createDimension('area',num_pts)\n", "            # Create variables\n", "            latitudes = dataset.createVariable('latitude',float,('latitude',))\n", "            longitudes = dataset.createVariable('longitude',float,('longitude',))\n", "            reals = dataset.createVariable('real',float,('real',))\n", "            imags = dataset.createVariable('imag',float,('imag',))\n", "            pareas = dataset.createVariable('area',float,('area',))\n", "            # Add units\n", "            latitudes.units = 'degree_north'\n", "            longitudes.units = 'degree_east'\n", "            reals.units = 'kg/m^2 (real part of load * load density)'\n", "            imags.units = 'kg/m^2 (imag part of load * load density)'\n", "            pareas.units = 'm^2 (unit area of patch * planet_radius^2)'\n", "            # Assign data\n", "            latitudes[:] = ilat\n", "            longitudes[:] = ilon\n", "            reals[:] = ic1\n", "            imags[:] = ic2\n", "            pareas[:] = iarea\n", "            # Write Data to File\n", "            dataset.close()\n\n", "        ## Rename file list\n", "        load_files = full_files.copy()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Make Sure Everyone Has Reported Back Before Moving On"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["comm.Barrier()"]}, {"cell_type": "markdown", "metadata": {}, "source": [" If Using a Common Mesh, Then Re-set the LoadFile Format to Indicate a Common Mesh is Used"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if (common_mesh == True):\n", "    loadfile_format = \"common\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["All Processors Get Certain Arrays and Parameters; Broadcast Them"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["lslat        = comm.bcast(lslat, root=0)\n", "lslon        = comm.bcast(lslon, root=0)\n", "lsmask       = comm.bcast(lsmask, root=0)\n", "load_files   = comm.bcast(load_files, root=0)\n", " \n", "# Gather the Processor Workloads for All Processors\n", "sendcounts = comm.gather(procN, root=0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Create a Data Type for the Convolution Results"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["cntype = MPI.DOUBLE.Create_contiguous(1)\n", "cntype.Commit()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Create a Data Type for Convolution Results for each Station and Load File"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["num_lfiles = len(load_files)\n", "ltype = MPI.DOUBLE.Create_contiguous(num_lfiles)\n", "ltype.Commit()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Set up suffix names"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["cnsuffixes = []"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Loop through Green's function files"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for dd in range(0,len(gffiles)):\n\n", "    # Current Green's functions\n", "    grn_file = gffiles[dd]\n\n", "    # Current filename extension\n", "    c_outstr = lngfext[dd]\n", "    c_outstr_noext = c_outstr[0:-4]\n\n", "    # Current suffix\n", "    csuffix = (rfm + \"_\" + loadfile_prefix + \"_\" + c_outstr_noext + outstr_conv + \".txt\")\n", "    cnsuffixes.append(csuffix)\n\n", "    # Scatter the Station Locations (By Index)\n", "    d_sub = np.empty((procN,))\n", "    comm.Scatterv([sta_idx, (sendcounts, None), cntype], d_sub, root=0)\n\n", "    # No need to Set up the arrays here; no need to use variables passed back from convolution\n", "    # We will just write out the files, and then read them in again later.\n\n", "    # Loop through the stations\n", "    for ii in range(0,len(d_sub)):\n", " \n", "        # Current station\n", "        current_sta = int(d_sub[ii]) # Index\n\n", "        # Remove Index If Only 1 Station\n", "        if (numel == 1): # only 1 station read in\n", "            csta = sta\n", "            clat = slat\n", "            clon = slon\n", "        else:\n", "            csta = sta[current_sta]\n", "            clat = slat[current_sta]\n", "            clon = slon[current_sta]\n\n", "        # If Rank is Main, Output Station Name\n", "        try:\n", "            csta = csta.decode()\n", "        except:\n", "            pass\n\n", "        # Output File Name\n", "        cnv_out = (csta + \"_\" + csuffix)\n\n", "        # Full file name\n", "        cn_fullpath = (cndirectory + cnprefix + cnv_out)\n\n", "        # Check if file already exists\n", "        if (os.path.isfile(cn_fullpath)):\n", "            if (overwrite == True): \n", "                os.remove(cn_fullpath) \n", "            else: \n", "                print(\":: File already exists: \" + cn_fullpath + \". Continuing...\")\n", "                continue\n\n", "        # Status update\n", "        print(':: Working on station: %s | Number: %6d of %6d | Rank: %6d' %(csta, (ii+1), len(d_sub), rank))\n\n", "        # Compute Convolution for Current File\n", "        eamp,epha,namp,npha,vamp,vpha = load_convolution.main(\\\n", "            grn_file,norm_flag,load_files,loadfile_format,regular,lslat,lslon,lsmask,lsmask_type,clat,clon,csta,cnv_out,load_density=ldens)\n", " \n", "    # No need to gather the data from MPI processors; no need to use variables passed back from convolution\n", "    # We will just write out the files, and then read them in again later.\n", "  \n", "# Free Data Type\n", "cntype.Free()\n", "ltype.Free()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Make Sure All Jobs Have Finished Before Continuing"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["comm.Barrier()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Remove load files that are no longer needed"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if (rank == 0):\n", "    if (common_mesh == True):\n", "        for gg in range(0,len(load_files)):\n", "            cfile = load_files[gg]\n", "            os.remove(cfile)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Make Sure All Jobs Have Finished Before Continuing"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["comm.Barrier()\n", " \n", "# ----------------- END CONVOLUTIONS ----------------------- #"]}, {"cell_type": "markdown", "metadata": {}, "source": ["-------------- BEGIN COMBINE STATIONS -------------------- #"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Only execute on main processor"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if (rank == 0):\n", " \n", "    # List of all combined filenames\n", "    combined_filenames = []\n", " \n", "    # Loop through model files\n", "    for ff in range(0,len(cnsuffixes)):\n\n", "        # Current convolution suffix\n", "        ccnsuffix = cnsuffixes[ff]\n\n", "        # Combine the stations into a single file\n", "        outdir_csta = (\"../output/CombineStations/\")\n", "        c_combined_filenames = combine_stations.main(cndirectory,cnprefix,ccnsuffix,output_directory=outdir_csta)\n\n", "        # Append to list\n", "        combined_filenames.append(c_combined_filenames)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Make sure all jobs have finished before continuing"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["comm.Barrier()  "]}, {"cell_type": "markdown", "metadata": {}, "source": ["-------------- END COMBINE STATIONS ---------------------- #"]}, {"cell_type": "markdown", "metadata": {}, "source": ["------------- BEGIN FINITE DIFFERENCE -------------------- #"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Only execute on main processor"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if (rank == 0):\n", " \n", "    # Take the difference between each perturbed displacement and the displacements predicted by the primary model (m0)\n", "    #  :: d(Gm)/dm\n", "    #  :: Separately for east, north, up at each station\n", "    #  :: [Gm' - Gm0] / [m' - m0]\n\n", "    # How many design matrices are we producing? (There will be one for each load model)\n", "    main_files = combined_filenames[-1]\n", "    main_files_out = []\n", " \n", "    # Loop through main files\n", "    for gg in range(0,len(main_files)):\n", " \n", "        # Current main file\n", "        mfile = main_files[gg]\n", "    \n", "        # Extract text of filename\n", "        mfilevals = mfile.split('/')\n", "        mfilename = mfilevals[-1]\n", "        mfilename = mfilename[0:-4]\n\n", "        # Read the main file\n", "        sta,lat,lon,eamp,epha,namp,npha,vamp,vpha = read_convolution_file.main(mfile)\n", " \n", "        # Convert from amplitude and phase to displacement\n", "        if (inc_imag == False): \n", "            edisp = np.multiply(eamp,np.cos(np.multiply(epha,(np.pi/180.))))\n", "            ndisp = np.multiply(namp,np.cos(np.multiply(npha,(np.pi/180.))))\n", "            udisp = np.multiply(vamp,np.cos(np.multiply(vpha,(np.pi/180.))))\n", "        # Convert Amp+Phase to Real+Imag\n", "        elif (inc_imag == True): \n", "            ere = np.multiply(eamp,np.cos(np.multiply(epha,(np.pi/180.))))\n", "            nre = np.multiply(namp,np.cos(np.multiply(npha,(np.pi/180.))))\n", "            ure = np.multiply(vamp,np.cos(np.multiply(vpha,(np.pi/180.))))\n", "            eim = np.multiply(eamp,np.sin(np.multiply(epha,(np.pi/180.))))\n", "            nim = np.multiply(namp,np.sin(np.multiply(npha,(np.pi/180.))))\n", "            uim = np.multiply(vamp,np.sin(np.multiply(vpha,(np.pi/180.))))\n", "        else: \n", "            sys.exit(':: Error: Incorrect selection for whether to include imaginary components. Must be True or False.')\n\n", "        # Export a simple text file of the original model\n", "        f_out_main = (\"startingmodel_\" + mfilename + \".txt\")\n", "        f_file_main = (\"../output/DesignMatrixStructure/\" + f_out_main)\n", "        main_files_out.append(f_file_main)\n", "        temp_head = (\"./temp_head_\" + str(np.random.randint(500)) + \".txt\")\n", "        temp_body = (\"./temp_body_\" + str(np.random.randint(500)) + \".txt\")\n", "        # Prepare Data for Output (as Structured Array)\n", "        if (inc_imag == False): \n", "            all_data = np.array(list(zip(sta,lat,lon,edisp,ndisp,udisp)), dtype=[('sta','U8'), \\\n", "                ('lat',float),('lon',float),('edisp',float),('ndisp',float),('udisp',float)])\n", "            # Write Header Info to File\n", "            hf = open(temp_head,'w')\n", "            temp_str = 'Station  Lat(+N,deg)  Lon(+E,deg)  E-Disp(mm)  N-Disp(mm)  U-Disp(mm)  \\n'\n", "            hf.write(temp_str)\n", "            hf.close()\n", "            # Write Model Results to File\n", "            np.savetxt(temp_body,all_data,fmt=[\"%s\"]+[\"%.7f\",]*5,delimiter=\"        \")\n", "        else:\n", "            all_data = np.array(list(zip(sta,lat,lon,ere,nre,ure,eim,nim,uim)), dtype=[('sta','U8'), \\\n", "                ('lat',float),('lon',float),('ere',float),('nre',float),('ure',float),('eim',float),('nim',float),('uim',float)])\n", "            # Write Header Info to File\n", "            hf = open(temp_head,'w')\n", "            temp_str = 'Station  Lat(+N,deg)  Lon(+E,deg)  E-Disp-Re(mm)  N-Disp-Re(mm)  U-Disp-Re(mm)  E-Disp-Im(mm)  N-Disp-Im(mm)  U-Disp-Im(mm)   \\n'\n", "            hf.write(temp_str)\n", "            hf.close()\n", "            # Write Model Results to File\n", "            np.savetxt(temp_body,all_data,fmt=[\"%s\"]+[\"%.7f\",]*8,delimiter=\"        \")\n", "        # Combine Header and Body Files\n", "        filenames_main = [temp_head, temp_body]\n", "        with open(f_file_main,'w') as outfile:\n", "            for fname in filenames_main:\n", "                with open(fname) as infile:\n", "                    outfile.write(infile.read())\n", "        # Remove Header and Body Files\n", "        os.remove(temp_head)\n", "        os.remove(temp_body) \n\n", "        # Set up current design matrix\n", "        if (inc_imag == False):\n", "            rowdim = len(sta)*3 # Multiply by three for the three spatial components (e,n,u)\n", "        else:\n", "            rowdim = len(sta)*6 # Multiply by six for the three spatial components (e,n,u), and real & imaginary components for each\n", "        coldim = len(combined_filenames)-1 # -1 so as not to include the main file (only the perturbations to structure; no. of depth ranges * 3 for mu,kappa,rho)\n", "        desmat = np.zeros((rowdim,coldim)) \n", "        dmrows = np.empty((rowdim,),dtype='U10') # Assumes that station names are no more than 9 characters in length (with E, N, or U also appended)\n", "        sclat = np.zeros((rowdim,))\n", "        sclon = np.zeros((rowdim,))\n", "        bottom_radius = np.zeros((coldim,))\n", "        top_radius = np.zeros((coldim,))\n", "        mat_param = np.empty((coldim,),dtype='U10')\n\n", "        # Loop through other files that correspond to this main file (perturbations to structure)\n", "        for hh in range(0,len(combined_filenames)-1): # -1 so as not to include the main file\n\n", "            # Current file with material perturbation\n", "            cpfiles = combined_filenames[hh]\n", "            cpfile = cpfiles[gg]\n\n", "            # Current depth bottom, depth top, and material parameter\n", "            clngfext = lngfext[hh] # information on current model parameter\n", "            clngfext_rmtxt = clngfext[0:-4] # remove the \".txt\" extension\n", "            perturbvars = clngfext_rmtxt.split('_')\n", "            bottom_radius[hh] = perturbvars[3]\n", "            top_radius[hh] = perturbvars[4]\n", "            mat_param[hh] = perturbvars[1]\n\n", "            # Read the current perturbed file\n", "            sta1,lat1,lon1,eamp1,epha1,namp1,npha1,vamp1,vpha1 = read_convolution_file.main(cpfile)\n\n", "            # Convert from amplitude and phase to displacement\n", "            if (inc_imag == False):\n", "                edisp1 = np.multiply(eamp1,np.cos(np.multiply(epha1,(np.pi/180.))))\n", "                ndisp1 = np.multiply(namp1,np.cos(np.multiply(npha1,(np.pi/180.))))\n", "                udisp1 = np.multiply(vamp1,np.cos(np.multiply(vpha1,(np.pi/180.))))\n", "            # Convert Amp+Phase to Real+Imag\n", "            else:\n", "                ere1 = np.multiply(eamp1,np.cos(np.multiply(epha1,(np.pi/180.))))\n", "                nre1 = np.multiply(namp1,np.cos(np.multiply(npha1,(np.pi/180.))))\n", "                ure1 = np.multiply(vamp1,np.cos(np.multiply(vpha1,(np.pi/180.))))\n", "                eim1 = np.multiply(eamp1,np.sin(np.multiply(epha1,(np.pi/180.))))\n", "                nim1 = np.multiply(namp1,np.sin(np.multiply(npha1,(np.pi/180.))))\n", "                uim1 = np.multiply(vamp1,np.sin(np.multiply(vpha1,(np.pi/180.))))\n\n", "            # Subtract displacements from those displacements in the main file\n", "            # And then divide by the perturbation. We want: dG(m)/dm, where dm=m'-m0\n", "            # In log space, m' = log10(m'_linear) and m0 = log10(m0_linear).\n", "            # To perturb the model parameters, we have: m' = m0 + \"perturbation\".\n", "            # Thus, perturbation = m' - m0, and we want to compute: dG(m)/perturbation.\n", "            # Hence, here, we compute the difference in displacement and divide by the perturbation.\n", "            if (inc_imag == False):\n", "                edisp_diff = np.divide(np.subtract(edisp1,edisp),perturbation)\n", "                ndisp_diff = np.divide(np.subtract(ndisp1,ndisp),perturbation)\n", "                udisp_diff = np.divide(np.subtract(udisp1,udisp),perturbation)\n", "            else:\n", "                ere_diff = np.divide(np.subtract(ere1,ere),perturbation)\n", "                nre_diff = np.divide(np.subtract(nre1,nre),perturbation)\n", "                ure_diff = np.divide(np.subtract(ure1,ure),perturbation)\n", "                eim_diff = np.divide(np.subtract(eim1,eim),perturbation)\n", "                nim_diff = np.divide(np.subtract(nim1,nim),perturbation)\n", "                uim_diff = np.divide(np.subtract(uim1,uim),perturbation)\n\n", "            # Loop through stations\n", "            for jj in range(0,len(sta1)): \n", " \n", "                # Fill in Design Matrix\n", "                if (inc_imag == False): \n", "                    idxe = (jj*3)+0\n", "                    idxn = (jj*3)+1\n", "                    idxu = (jj*3)+2\n", "                    desmat[idxe,hh] = edisp_diff[jj]\n", "                    desmat[idxn,hh] = ndisp_diff[jj]\n", "                    desmat[idxu,hh] = udisp_diff[jj]\n", "                    dmrows[idxe] = (sta1[jj] + 'E')\n", "                    dmrows[idxn] = (sta1[jj] + 'N')\n", "                    dmrows[idxu] = (sta1[jj] + 'U')\n", "                    sclat[idxe] = lat1[jj]\n", "                    sclat[idxn] = lat1[jj]\n", "                    sclat[idxu] = lat1[jj]\n", "                    sclon[idxe] = lon1[jj]\n", "                    sclon[idxn] = lon1[jj]\n", "                    sclon[idxu] = lon1[jj]\n", "                else:\n", "                    idxere = (jj*6)+0\n", "                    idxnre = (jj*6)+1\n", "                    idxure = (jj*6)+2\n", "                    idxeim = (jj*6)+3\n", "                    idxnim = (jj*6)+4\n", "                    idxuim = (jj*6)+5\n", "                    desmat[idxere,hh] = ere_diff[jj]\n", "                    desmat[idxnre,hh] = nre_diff[jj]\n", "                    desmat[idxure,hh] = ure_diff[jj]\n", "                    desmat[idxeim,hh] = eim_diff[jj]\n", "                    desmat[idxnim,hh] = nim_diff[jj]\n", "                    desmat[idxuim,hh] = uim_diff[jj]\n", "                    dmrows[idxere] = (sta1[jj] + 'Ere')\n", "                    dmrows[idxnre] = (sta1[jj] + 'Nre')\n", "                    dmrows[idxure] = (sta1[jj] + 'Ure')\n", "                    dmrows[idxeim] = (sta1[jj] + 'Eim')\n", "                    dmrows[idxnim] = (sta1[jj] + 'Nim')\n", "                    dmrows[idxuim] = (sta1[jj] + 'Uim')\n", "                    sclat[idxere] = lat1[jj]\n", "                    sclat[idxnre] = lat1[jj]\n", "                    sclat[idxure] = lat1[jj]\n", "                    sclon[idxere] = lon1[jj]\n", "                    sclon[idxnre] = lon1[jj]\n", "                    sclon[idxure] = lon1[jj]\n", "                    sclat[idxeim] = lat1[jj]\n", "                    sclat[idxnim] = lat1[jj]\n", "                    sclat[idxuim] = lat1[jj]\n", "                    sclon[idxeim] = lon1[jj]\n", "                    sclon[idxnim] = lon1[jj]\n", "                    sclon[idxuim] = lon1[jj]\n\n", "        # Write Design Matrix to File\n", "        print(\":: \")\n", "        print(\":: \")\n", "        print(\":: Writing netCDF-formatted file.\")\n", "        f_out = (\"designmatrix_\" + mfilename + \".nc\")\n", "        f_file = (\"../output/DesignMatrixStructure/\" + f_out)\n", "        # Check if file already exists; if so, delete existing file\n", "        if (os.path.isfile(f_file)):\n", "            os.remove(f_file)\n", "        # Open new NetCDF file in \"write\" mode\n", "        dataset = netCDF4.Dataset(f_file,'w',format='NETCDF4_CLASSIC')\n", "        # Define dimensions for variables\n", "        desmat_shape = desmat.shape\n", "        num_rows = desmat_shape[0]\n", "        num_cols = desmat_shape[1]\n", "        nstacomp = dataset.createDimension('nstacomp',num_rows)\n", "        nstructure = dataset.createDimension('nstructure',num_cols)\n", "        nchars = dataset.createDimension('nchars',10)\n", "        # Create variables\n", "        sta_comp_id = dataset.createVariable('sta_comp_id','S1',('nstacomp','nchars'))\n", "        design_matrix = dataset.createVariable('design_matrix',float,('nstacomp','nstructure'))\n", "        sta_comp_lat = dataset.createVariable('sta_comp_lat',float,('nstacomp',))\n", "        sta_comp_lon = dataset.createVariable('sta_comp_lon',float,('nstacomp',))\n", "        perturb_radius_bottom = dataset.createVariable('perturb_radius_bottom',float,('nstructure',))\n", "        perturb_radius_top = dataset.createVariable('perturb_radius_top',float,('nstructure',))\n", "        perturb_param = dataset.createVariable('perturb_param','S1',('nstructure','nchars'))\n", "        # Add units\n", "        sta_comp_id.units = 'string'\n", "        if (inc_imag == False): \n", "            sta_comp_id.long_name = 'station_component_id'\n", "        else: \n", "            sta_comp_id.long_name = 'station_component_RealImaginary_id'\n", "        design_matrix.units = 'mm'\n", "        design_matrix.long_name = 'displacement_mm'\n", "        sta_comp_lat.units = 'degrees_north'\n", "        sta_comp_lat.long_name = 'station_latitude'\n", "        sta_comp_lon.units = 'degrees_east'\n", "        sta_comp_lon.long_name = 'station_longitude'\n", "        perturb_radius_bottom.units = 'km'\n", "        perturb_radius_bottom.long_name = 'bottom_of_perturbed_layer'\n", "        perturb_radius_top.units = 'km'\n", "        perturb_radius_top.long_name = 'top_of_perturbed_layer'\n", "        perturb_param.units = 'string'\n", "        perturb_param.long_name = 'material_parameter_perturbed'\n", "        # Assign data\n", "        #  https://unidata.github.io/netcdf4-python/ (see \"Dealing with Strings\")\n", "        #  sta_comp_id[:] = netCDF4.stringtochar(np.array(dmrows,dtype='S10'))\n", "        sta_comp_id._Encoding = 'ascii'\n", "        sta_comp_id[:] = np.array(dmrows,dtype='S10')\n", "        design_matrix[:,:] = desmat\n", "        sta_comp_lat[:] = sclat\n", "        sta_comp_lon[:] = sclon\n", "        perturb_radius_bottom[:] = bottom_radius\n", "        perturb_radius_top[:] = top_radius\n", "        perturb_param._Encoding = 'ascii'\n", "        perturb_param[:] = np.array(mat_param,dtype='S10')\n", "    \n", "        # Write Data to File\n", "        dataset.close()\n\n", "        # Print the output filename\n", "        print(f_file)\n\n", "        # Read the netCDF file as a test\n", "        f = netCDF4.Dataset(f_file)\n", "        #print(f.variables)\n", "        sta_comp_ids = f.variables['sta_comp_id'][:]\n", "        design_matrix = f.variables['design_matrix'][:]\n", "        sta_comp_lat = f.variables['sta_comp_lat'][:]\n", "        sta_comp_lon = f.variables['sta_comp_lon'][:]\n", "        perturb_radius_bottom = f.variables['perturb_radius_bottom'][:]\n", "        perturb_radius_top = f.variables['perturb_radius_top'][:]\n", "        perturb_param = f.variables['perturb_param'][:]\n", "        f.close()\n\n", "    # Remind users that they will also need the original forward models when they run the inversion:\n", "    print(':: ')\n", "    print(':: ')\n", "    print(':: Reminder: You will also need the original forward model when running the inversion. [d-(Gm0)] = [d(Gm)/dm]*[dm]')\n", "    print('::   (Gm0) represents the original forward model. [d-(Gm0)] represents the residual vector between GPS data and the original forward model')\n", "    print('::   [d(Gm)/dm] represents the perturbations to the surface displacements with a perturbation to each model parameter.')\n", "    print('::      It is the design matrix computed here. The default perturbation is 1%.')\n", "    print('::   [dm] represents the model vector to be solved for in the inversion.')\n", "    print('::      It is the perturbation to each model parameter required to best fit the residual data.')\n", "    print(':: The original forward model(s) are: ')\n", "    print(main_files)\n", "    print(':: And the original forward model(s) recast into real and imaginary components are: ')\n", "    print(main_files_out)\n", "    print(':: ')\n", "    print(':: ')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["-------------- END FINITE DIFFERENCE --------------------- #"]}, {"cell_type": "markdown", "metadata": {}, "source": ["--------------------- END CODE --------------------------- #"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}