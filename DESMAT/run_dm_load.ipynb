{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#!/usr/bin/env python"]}, {"cell_type": "markdown", "metadata": {}, "source": ["*********************************************************************<br>\n", "MAIN PROGRAM TO COMPUTE A DESIGN MATRIX TO INVERT FOR SURFACE LOAD --<br>\n", "BY CONVOLVING DISPLACEMENT LOAD GREENS FUNCTIONS WITH A UNIFORM LOAD IN <br>\n", "EACH USER-DEFINED GRID CELL <br>\n", "<br>\n", "Copyright (c) 2021-2024: HILARY R. MARTENS<br>\n", "<br>\n", "This file is part of LoadDef.<br>\n", "<br>\n", "   LoadDef is free software: you can redistribute it and/or modify<br>\n", "   it under the terms of the GNU General Public License as published by<br>\n", "   the Free Software Foundation, either version 3 of the License, or<br>\n", "   any later version.<br>\n", "<br>\n", "   LoadDef is distributed in the hope that it will be useful,<br>\n", "   but WITHOUT ANY WARRANTY; without even the implied warranty of<br>\n", "   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the<br>\n", "   GNU General Public License for more details.<br>\n", "<br>\n", "   You should have received a copy of the GNU General Public License<br>\n", "   along with LoadDef.  If not, see <https://www.gnu.org/licenses/>.<br>\n", "<br>\n", "*********************************************************************"]}, {"cell_type": "markdown", "metadata": {}, "source": ["IMPORT PRINT FUNCTION"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from __future__ import print_function"]}, {"cell_type": "markdown", "metadata": {}, "source": ["IMPORT MPI MODULE"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from mpi4py import MPI"]}, {"cell_type": "markdown", "metadata": {}, "source": ["MODIFY PYTHON PATH TO INCLUDE 'LoadDef' DIRECTORY"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import sys\n", "import os\n", "sys.path.append(os.getcwd() + \"/../\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["IMPORT PYTHON MODULES"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import scipy as sc\n", "import datetime\n", "import netCDF4 \n", "from math import pi\n", "from scipy import interpolate\n", "from CONVGF.utility import read_station_file\n", "from CONVGF.utility import read_lsmask\n", "from CONVGF.utility import read_greens_fcn_file\n", "from CONVGF.utility import read_greens_fcn_file_norm\n", "from CONVGF.utility import normalize_greens_fcns\n", "from CONVGF.CN import load_convolution\n", "from CONVGF.CN import compute_specific_greens_fcns\n", "from CONVGF.CN import generate_integration_mesh\n", "from CONVGF.CN import intmesh2geogcoords\n", "from CONVGF.CN import integrate_greens_fcns\n", "from CONVGF.CN import compute_angularDist_azimuth\n", "from CONVGF.CN import interpolate_lsmask\n", "from CONVGF.CN import coef2amppha\n", "import matplotlib.pyplot as plt\n", "import matplotlib.cm as cm"]}, {"cell_type": "markdown", "metadata": {}, "source": ["--------------- SPECIFY USER INPUTS --------------------- #"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Reference Frame (used for filenames) [Blewitt 2003]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["rfm = \"cf\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Greens Function File<br>\n", " :: May be load Green's function file output directly from run_gf.py (norm_flag = False)<br>\n", " :: May be from a published table, normalized according to Farrell (1972) conventions [theta, u_norm, v_norm]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["pmod = \"PREM\"\n", "grn_file = (\"../output/Greens_Functions/\" + rfm + \"_\" + pmod + \".txt\")\n", "norm_flag  = False"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Full Path to Grid File Containing Cells<br>\n", " :: Format: south lat [float], north lat [float], west lon [float], east lon [float], unique cell id [string]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["cellfname = (\"cells_28.0_50.0_233.0_258.0_0.25\")\n", "loadgrid  = (\"../output/Grid_Files/nc/cells/\" + cellfname + \".nc\") "]}, {"cell_type": "markdown", "metadata": {}, "source": ["NEW OPTION: Provide a common geographic mesh? <br>\n", "If True, must provide the full path to a mesh file (see: GRDGEN/common_mesh). <br>\n", "If False, a station-centered grid will be created within the functions called here. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["common_mesh = True\n", "# Full Path to Grid File Containing Surface Mesh (for sampling the load Green's functions)\n", "#  :: Format: latitude midpoints [float,degrees N], longitude midpoints [float,degrees E], unit area of each patch [float,dimensionless (need to multiply by r^2)]\n", "meshfname = (\"commonMesh_regional_28.0_50.0_233.0_258.0_0.01_0.01_oceanmask\")\n", "convmesh = (\"../output/Grid_Files/nc/commonMesh/\" + meshfname + \".nc\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Planet Radius (in meters; used for Greens function normalization)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["planet_radius = 6371000."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Load Density<br>\n", " Recommended: 1000 kg/m^3 as a standard for water-mass inversion"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ldens = 1000.0"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Ocean/Land Mask<br>\n", " :: 0 = do not mask ocean or land (retain full model); 1 = mask out land (retain ocean); 2 = mask out oceans (retain land)<br>\n", " :: Recommended: 1 for oceanic; 2 for atmospheric and continental water<br>\n", " :: When pre-generating a common mesh, a land-sea mask can be applied a priori to the mesh. If that is done, it is recommended to set lsmask_type = 0 here. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["lsmask_type = 0"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Full Path to Land-Sea Mask File (May be Irregular and Sparse)<br>\n", " :: Format: Lat, Lon, Mask [0=ocean; 1=land]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["lsmask_file = (\"../input/Land_Sea/ETOPO1_Ice_g_gmt4_wADD.txt\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Station/Grid-Point Location File (Lat, Lon, StationName)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sta_file_name = (\"NOTA_Select\")\n", "sta_file = (\"../input/Station_Locations/\" + sta_file_name + \".txt\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Optional: Additional string to include in output filenames (e.g. \"_2019\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if (common_mesh == True):\n", "    mtag = meshfname\n", "else:\n", "    mtag = \"stationMesh\"\n", "outstr = (pmod + \"_\" + cellfname + \"_\" + mtag + \"_\" + sta_file_name)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["------------------ END USER INPUTS ----------------------- #"]}, {"cell_type": "markdown", "metadata": {}, "source": ["-------------------- SETUP MPI --------------------------- #"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Get the Main MPI Communicator That Controls Communication Between Processors"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["comm = MPI.COMM_WORLD\n", "# Get My \"Rank\", i.e. the Processor Number Assigned to Me\n", "rank = comm.Get_rank()\n", "# Get the Total Number of Other Processors Used\n", "size = comm.Get_size()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---------------------------------------------------------- #"]}, {"cell_type": "markdown", "metadata": {}, "source": ["-------------------- BEGIN CODE -------------------------- #"]}, {"cell_type": "markdown", "metadata": {}, "source": ["LoadFile Format (\"bbox\" tells the software to read the text file line by line for individual bounding-box cells in the load grid)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["loadfile_format = \"bbox\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Bounding boxes for grid cells are regular in lat/lon"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["regular = True"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Check for existence of load grid"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if not os.path.isfile(loadgrid):\n", "    sys.exit('Error: The load grid does not exist. You may need to create it. See GRDGEN/design_matrix/ .')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Put loadgrid file into a list (for consistency with how traditional load files are treated)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["load_files = []\n", "load_files.append(loadgrid)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Ensure that the Output Directories Exist"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if (rank == 0):\n", "    if not (os.path.isdir(\"../output/Convolution/\")):\n", "        os.makedirs(\"../output/Convolution/\")\n", "    if not (os.path.isdir(\"../output/DesignMatrixLoad/\")):\n", "        os.makedirs(\"../output/DesignMatrixLoad/\")\n", "    if not (os.path.isdir(\"../output/Figures/\")):\n", "        os.makedirs(\"../output/Figures/\")\n\n", "    # Read Station File\n", "    slat,slon,sta = read_station_file.main(sta_file)\n\n", "    # Ensure that Station Locations are in Range 0-360\n", "    neglon_idx = np.where(slon<0.)\n", "    slon[neglon_idx] += 360.\n\n", "    # Determine Number of Stations Read In\n", "    if isinstance(slat,float) == True: # only 1 station\n", "        numel = 1\n", "    else:\n", "        numel = len(slat)\n\n", "    # Generate an Array of File Indices\n", "    sta_idx = np.linspace(0,numel,num=numel,endpoint=False)\n", "    np.random.shuffle(sta_idx)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["else: # If I'm a worker, I know nothing yet about the data\n", "    slat = slon = sta = numel = sta_idx = None"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Make Sure Everyone Has Reported Back Before Moving On"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["comm.Barrier()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["All Processors Get Certain Arrays and Parameters; Broadcast Them"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sta          = comm.bcast(sta, root=0)\n", "slat         = comm.bcast(slat, root=0)\n", "slon         = comm.bcast(slon, root=0)\n", "numel        = comm.bcast(numel, root=0)\n", "sta_idx      = comm.bcast(sta_idx, root=0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["MPI: Determine the Chunk Sizes for the Convolution"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["total_stations = len(slat)\n", "nominal_load = total_stations // size # Floor Divide\n", "# Final Chunk Might Be Different in Size Than the Nominal Load\n", "if rank == size - 1:\n", "    procN = total_stations - rank * nominal_load\n", "else:\n", "    procN = nominal_load"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Make some preparations that are common to all stations"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if (rank == 0): \n\n", "    # Read in the Land-Sea Mask\n", "    if (lsmask_type > 0):\n", "        lslat,lslon,lsmask = read_lsmask.main(lsmask_file)\n", "    else:\n", "        # Doesn't really matter so long as there are some values filled in with something other than 1 or 2\n", "        lat1d = np.arange(-90.,90.,5.)\n", "        lon1d = np.arange(0.,360.,5.)\n", "        olon,olat = np.meshgrid(lon1d,lat1d)\n", "        lslat = olat.flatten()\n", "        lslon = olon.flatten()\n", "        lsmask = np.ones((len(lslat),)) * -1.\n\n", "    # Ensure that Land-Sea Mask Longitudes are in Range 0-360\n", "    neglon_idx = np.where(lslon<0.)\n", "    lslon[neglon_idx] += 360.\n", " \n", "    # Read in the loadgrid\n", "    #  Here, used for determining and writing out the center of each load cell\n", "    lcext = loadgrid[-2::]\n", "    if (lcext == 'xt'):\n", "        load_cells = np.loadtxt(loadgrid,usecols=(4,),unpack=True,dtype='U')\n", "        lcslat,lcnlat,lcwlon,lcelon = np.loadtxt(loadgrid,usecols=(0,1,2,3),unpack=True)\n", "    elif (lcext == 'nc'):\n", "        f = netCDF4.Dataset(loadgrid)\n", "        load_cells = f.variables['cell_ids'][:]\n", "        lcslat = f.variables['slatitude'][:]\n", "        lcnlat = f.variables['nlatitude'][:]\n", "        lcwlon = f.variables['wlongitude'][:]\n", "        lcelon = f.variables['elongitude'][:]\n", "        f.close()    \n", "    # Ensure that Bounding Box Longitudes are in Range 0-360\n", "    for yy in range(0,len(lcwlon)):\n", "        if (lcwlon[yy] < 0.):\n", "            lcwlon[yy] += 360.\n", "        if (lcelon[yy] < 0.):\n", "            lcelon[yy] += 360.\n", "    # Compute center of each load cell\n", "    print(':: Warning: Computing center of load cells. Special consideration should be made for cells spanning the prime meridian, if applicable.')\n", "    # For prime meridian, should shift longitude range to [-180,180]. Still need to do this...\n", "    lclat = (lcslat + lcnlat)/2.\n", "    lclon = (lcwlon + lcelon)/2.\n\n", "    # Read in the common convolution mesh, if applicable\n", "    if (common_mesh == True): \n", "        print(':: Common Mesh True. Reading in ilat, ilon, iarea.')\n", "        lcext = convmesh[-2::]\n", "        if (lcext == 'xt'):\n", "            ilat,ilon,unit_area = np.loadtxt(convmesh,usecols=(0,1,2),unpack=True)\n", "            iarea = np.multiply(unit_area, planet_radius**2) # convert from unit area to true area of the spherical patch in m^2\n", "        elif (lcext == 'nc'):\n", "            f = netCDF4.Dataset(convmesh)\n", "            ilat = f.variables['midpoint_lat'][:]\n", "            ilon = f.variables['midpoint_lon'][:]\n", "            unit_area = f.variables['unit_area_patch'][:]\n", "            f.close()\n", "            iarea = np.multiply(unit_area, planet_radius**2) # convert from unit area to true area of the spherical patch in m^2\n", "    else:\n", "        ilat = ilon = iarea = None\n\n", "    # Determine the Land-Sea Mask: Interpolate onto Mesh\n", "    if (common_mesh == True): \n", "        print(':: Common Mesh True. Applying Land-Sea Mask.')\n", "        print(':: Number of Grid Points: %s | Size of LSMask: %s' %(str(len(ilat)), str(lsmask.shape)))\n", "        lsmk = interpolate_lsmask.main(ilat,ilon,lslat,lslon,lsmask)\n", "        print(':: Finished LSMask Interpolation.')\n", "    else:\n", "        lsmk = None\n\n", "    # For a common mesh, can pre-determine the mesh points within each cell used for the inversion.\n", "    # Also, apply the land-sea mask.\n", "    if (common_mesh == True):\n", "        ## Check load file format\n", "        if not (loadfile_format == \"bbox\"): # list of cells, rather than traditional load files\n", "            sys.exit(':: Error -- The loadfile format should be bbox for generating the design matrix for surface load distribution.')\n", "        ## Prepare land-sea mask application\n", "        if (lsmask_type == 2): \n", "            test_elements = np.where(lsmk == 0); test_elements = test_elements[0]\n", "        elif (lsmask_type == 1): \n", "            test_elements = np.where(lsmk == 1); test_elements = test_elements[0]\n", "        ## Select the Appropriate Cell ID\n", "        count = 0 # initialize figure counter (don't want to get stuck in huge loop plotting figures...)\n", "        colvals = np.empty((len(lclat),),dtype=object) # initialize array that will hold lists of column indices (corresponding to LGF mesh points in each grid cell)\n", "        for qq in range(0,len(lclat)):\n", "            clc = load_cells[qq]\n", "            cslat = lcslat[qq]\n", "            cnlat = lcnlat[qq]\n", "            cwlon = lcwlon[qq]\n", "            celon = lcelon[qq]\n", "            ## Find ilat and ilon within cell\n", "            yes_idx = np.where((ilat >= cslat) & (ilat <= cnlat) & (ilon >= cwlon) & (ilon <= celon)); yes_idx = yes_idx[0]\n", "            print(':: Number of convolution grid points within load cell ', clc, ' of ', len(lclat), ': ', len(yes_idx))\n", "            ## Apply land-sea mask\n", "            if (lsmask_type == 2): \n", "                mask = np.isin(yes_idx, test_elements, assume_unique=True)\n", "                idx_to_delete = np.nonzero(mask)\n", "                yes_idx = np.delete(yes_idx,idx_to_delete)\n", "            elif (lsmask_type == 1): \n", "                mask = np.isin(yes_idx, test_elements, assume_unique=True)\n", "                idx_to_delete = np.nonzero(mask)\n", "                yes_idx = np.delete(yes_idx,idx_to_delete)\n", "            ## Write indices within cell to the main array\n", "            colvals[qq] = yes_idx.tolist()\n", "            #### OPTIONAL: Plot the load cell\n", "            #### To suppress plotting, set max_count to 0\n", "            #max_count = 0\n", "            #if (len(yes_idx)>0):\n", "            #    if (count < max_count):\n", "            #        plot_fig = True\n", "            #        count += 1\n", "            #        print(':: Figure count: ', count)\n", "            #    else:\n", "            #        plot_fig = False\n", "            #else:\n", "            #    plot_fig = False\n", "            #if plot_fig:\n", "            #    print(':: Plotting the load cell. [run_dm_load.py]')\n", "            #    cslat_plot = cslat - 0.5\n", "            #    cnlat_plot = cnlat + 0.5\n", "            #    cwlon_plot = cwlon - 0.5\n", "            #    celon_plot = celon + 0.5\n", "            #    idx_plot = np.where((ilon >= cwlon_plot) & (ilon <= celon_plot) & (ilat >= cslat_plot) & (ilat <= cnlat_plot)); idx_plot = idx_plot[0]\n", "            #    ilon_plot = ilon[idx_plot]\n", "            #    ilat_plot = ilat[idx_plot]\n", "            #    ic1_plot = cic1[idx_plot]\n", "            #    plt.scatter(ilon_plot,ilat_plot,c=ic1_plot,s=1,cmap=cm.BuPu)\n", "            #    plt.colorbar(orientation='horizontal')\n", "            #    fig_name = (\"../output/Figures/\" + str(cslat) + \"_\" + str(cnlat) + \"_\" + str(cwlon) + \"_\" + str(celon) + \".png\")\n", "            #    plt.savefig(fig_name,format=\"png\")\n", "            #    #plt.show()\n", "            #    plt.close()\n", "    else:\n", "        colvals = None\n\n", "    # Initialize Arrays\n", "    numcells = len(lclat)\n", "    eamp = np.empty((numel,numcells))\n", "    epha = np.empty((numel,numcells))\n", "    namp = np.empty((numel,numcells))\n", "    npha = np.empty((numel,numcells))\n", "    vamp = np.empty((numel,numcells))\n", "    vpha = np.empty((numel,numcells))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["If I'm a Worker, I Know Nothing About the Data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["else:\n", "    load_cells = lclat = lclon = lslat = lslon = lsmask = None \n", "    ilat = ilon = iarea = lsmk = colvals = numcells = None\n", "    eamp = epha = namp = npha = vamp = vpha = None"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Make Sure Everyone Has Reported Back Before Moving On"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["comm.Barrier()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["All Processors Get Certain Arrays and Parameters; Broadcast Them"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["load_cells  = comm.bcast(load_cells, root=0)\n", "lclat       = comm.bcast(lclat, root=0)\n", "lclon       = comm.bcast(lclon, root=0)\n", "lslat       = comm.bcast(lslat, root=0)\n", "lslon       = comm.bcast(lslon, root=0)\n", "lsmask      = comm.bcast(lsmask, root=0)\n", "ilat        = comm.bcast(ilat, root=0)\n", "ilon        = comm.bcast(ilon, root=0)\n", "iarea       = comm.bcast(iarea, root=0)\n", "lsmk        = comm.bcast(lsmk, root=0)\n", "colvals     = comm.bcast(colvals, root=0)\n", "numcells    = comm.bcast(numcells, root=0)\n", "eamp         = comm.bcast(eamp, root=0)\n", "epha         = comm.bcast(epha, root=0)\n", "namp         = comm.bcast(namp, root=0)\n", "npha         = comm.bcast(npha, root=0)\n", "vamp         = comm.bcast(vamp, root=0)\n", "vpha         = comm.bcast(vpha, root=0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Gather the Processor Workloads for All Processors"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sendcounts = comm.gather(procN, root=0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Create a Data Type for the Convolution Results"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["cntype = MPI.DOUBLE.Create_contiguous(1)\n", "cntype.Commit()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Create a Data Type for Convolution Results for each Station and Load File"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ltype = MPI.DOUBLE.Create_contiguous(numcells)\n", "ltype.Commit()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Scatter the Station Locations (By Index)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["d_sub = np.empty((procN,))\n", "comm.Scatterv([sta_idx, (sendcounts, None), cntype], d_sub, root=0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Set up the arrays"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["eamp_sub = np.empty((len(d_sub),numcells))\n", "epha_sub = np.empty((len(d_sub),numcells))\n", "namp_sub = np.empty((len(d_sub),numcells))\n", "npha_sub = np.empty((len(d_sub),numcells))\n", "vamp_sub = np.empty((len(d_sub),numcells))\n", "vpha_sub = np.empty((len(d_sub),numcells))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Set up Design matrix (rows = stations[e,n,u]; columns = load cells)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if (rank == 0):\n", "    desmat = np.zeros((numel*3, numcells)) # Multiplication by 3 for 3 spatial dimensions (e,n,u)\n", "    dmrows = np.empty((numel*3,),dtype='U10') # Assumes that station names are no more than 9 characters in length (with E, N, or U also appended)\n", "    sclat = np.zeros((numel*3,))\n", "    sclon = np.zeros((numel*3,))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Loop Through Each Station"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for ii in range(0,len(d_sub)):\n", " \n", "    # Current station\n", "    current_sta = int(d_sub[ii]) # Index\n\n", "    # Remove Index If Only 1 Station\n", "    if (numel == 1): # only 1 station read in\n", "        csta = sta\n", "        clat = slat\n", "        clon = slon\n", "    else:\n", "        csta = sta[current_sta]\n", "        clat = slat[current_sta]\n", "        clon = slon[current_sta]\n\n", "    # If Rank is Main, Output Station Name\n", "    try:\n", "        csta = csta.decode()\n", "    except:\n", "        pass\n\n", "    # Output File Name\n", "    cnv_out = csta + \"_\" + rfm + \"_\" + outstr + \".txt\"\n\n", "    # Status update\n", "    print(':: Working on station: %s | Number: %6d of %6d | Rank: %6d' %(csta, (ii+1), len(d_sub), rank))\n\n", "    # If using a common mesh, then integrate the Green's Functions and sum up all the cells\n", "    if (common_mesh == True):\n", "        print(':: Common Mesh True. Computing specific Greens functions for common mesh.')\n", "        # Read in the Green's Functions\n", "        if norm_flag == True:\n", "            theta,u,v,unormFarrell,vnormFarrell = read_greens_fcn_file_norm.main(grn_file,rad)\n", "        else:\n", "            theta,u,v,unormFarrell,vnormFarrell = read_greens_fcn_file.main(grn_file)\n", "        # Normalize Green's According to Farrell Convention\n", "        nfactor = 1E12*planet_radius\n", "        unorm = np.multiply(u,theta) * nfactor\n", "        vnorm = np.multiply(v,theta) * nfactor\n", "        # Interpolate Green's Functions\n", "        tck_gfu = interpolate.splrep(theta,unorm,k=3)\n", "        tck_gfv = interpolate.splrep(theta,vnorm,k=3)\n", "        # Find Great-Circle Distances between Station and Grid Points in the Common Mesh\n", "        delta,haz = compute_angularDist_azimuth.main(clat,clon,ilat,ilon)\n", "        # Compute Integrated Greens Functions\n", "        gfu = interpolate.splev(delta,tck_gfu,der=0)\n", "        gfv = interpolate.splev(delta,tck_gfv,der=0)\n", "        uint = iarea * gfu\n", "        vint = iarea * gfv\n", "        # Un-normalize\n", "        uint = np.divide(uint,delta) / nfactor\n", "        vint = np.divide(vint,delta) / nfactor\n", "        # Compute Greens Functions Specific to Receiver and Grid (Geographic Coordinates)\n", "        # Per the small-angle approximation, when the width of the cell is small, then the integrals over the horizontal displacement response\n", "        #  reduce to [-beta*cos(alpha)] for the north component and [-beta*sin(alpha)] for the east component. \n", "        #  See equations 4.221 and 4.222 in H.R. Martens (2016, Caltech thesis). Here, the T(alpha) function is included in the integration. \n", "        #  When we use a common mesh, the convolution mesh is no longer symmetric about the station; therefore, it does not make sense to \n", "        #  include T(alpha) in the integration. However, we can see that the T(alpha) function can be moved outside the integral when the \n", "        #  value of beta is small (and we can invoke the small-angle approximation): 2*sin(beta/2) reduces to 2*(beta/2) = beta.\n", "        #  In this way, we treat the horizontal-component integration in the same way as the vertical component:\n", "        #  Integrate over the area of each patch (area element on a sphere): int_theta int_phi r^2 sin(theta) d(theta) d(phi)\n", "        #   where theta is co-latitude and phi is azimuth in a geographic coordinate system.\n", "        #  Next, we can multiply the horizontal solutions by [-cos(alpha)] and [-sin(alpha)] to convert to north and east components, respectively,\n", "        #   where alpha is the azimuthal angle between the north pole and the load point, as subtended by the station, and as measured clockwise from north. \n", "        ur,ue,un = compute_specific_greens_fcns.main(haz,uint,vint)\n", "        print(':: Common Mesh True. Summing up integrated and specific LGFs within each cell directly.')\n", "        ec2 = 0 # imaginary component is zero for non-periodic load\n", "        nc2 = 0 # imaginary component is zero for non-periodic load\n", "        vc2 = 0 # imaginary component is zero for non-periodic load\n", "        # For each station, must sum up the specific LGFs and load values for every inversion cell\n", "        for dd in range(0,numcells): # loop through load cells used in the inversion grid\n", "            mycols = colvals[dd] # Find indices of Greens-function mesh points within the current load cell\n", "            if (len(mycols) == 0): # Nothing in this cell; assign everything to zero.\n", "                # Assign values to appropriate amplitude arrays\n", "                eamp_sub[ii,dd] = 0.\n", "                epha_sub[ii,dd] = 0.\n", "                namp_sub[ii,dd] = 0.\n", "                npha_sub[ii,dd] = 0.\n", "                vamp_sub[ii,dd] = 0.\n", "                vpha_sub[ii,dd] = 0.\n", "            else: # Sum up the contributions from each surface patch within the current load cell\n", "                ec1 = np.sum(ue[mycols])*ldens # Sum up all the relevant integrated and specific Greens functions (east); and then multiply by load density\n", "                nc1 = np.sum(un[mycols])*ldens # Sum up all the relevant integrated and specific Greens functions (north); and then multiply by load density\n", "                vc1 = np.sum(ur[mycols])*ldens # Sum up all the relevant integrated and specific Greens functions (up); and then multiply by load density\n", "                # Convert Coefficients to Amplitude and Phase\n", "                # Note: Conversion from meters to mm also happens here!\n", "                ceamp,cepha,cnamp,cnpha,cvamp,cvpha = coef2amppha.main(ec1,ec2,nc1,nc2,vc1,vc2)\n", "                # Assign values to appropriate amplitude arrays\n", "                eamp_sub[ii,dd] = ceamp\n", "                epha_sub[ii,dd] = cepha\n", "                namp_sub[ii,dd] = cnamp\n", "                npha_sub[ii,dd] = cnpha\n", "                vamp_sub[ii,dd] = cvamp\n", "                vpha_sub[ii,dd] = cvpha\n\n", "    # If not using a common mesh, then set up a station-centered grid and run the convolution as normal\n", "    else:\n", "        # For a station-centered grid, we cannot pre-determine the grid points within each cell, so we will send information to another function. \n", "        #### NOTE: Mesh defaults are adjusted to ensure we get a good number of points within each grid cell to adequately represent the shape of each cell.\n", "        print(':: Common Mesh False. Performing the standard convolution.')\n", "        # Compute Convolution for Current File\n", "        eamp_sub[ii,:],epha_sub[ii,:],namp_sub[ii,:],npha_sub[ii,:],vamp_sub[ii,:],vpha_sub[ii,:] = load_convolution.main(\\\n", "            grn_file,norm_flag,load_files,loadfile_format,regular,lslat,lslon,lsmask,lsmask_type,\\\n", "            clat,clon,csta,cnv_out,load_density=ldens,azminc=0.5,delinc3=0.005,delinc4=0.02,delinc5=0.05)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Make Sure All Jobs Have Finished Before Continuing"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["comm.Barrier()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Gather Results"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["comm.Gatherv(eamp_sub, [eamp, (sendcounts, None), ltype], root=0)\n", "comm.Gatherv(epha_sub, [epha, (sendcounts, None), ltype], root=0)\n", "comm.Gatherv(namp_sub, [namp, (sendcounts, None), ltype], root=0)\n", "comm.Gatherv(npha_sub, [npha, (sendcounts, None), ltype], root=0)\n", "comm.Gatherv(vamp_sub, [vamp, (sendcounts, None), ltype], root=0)\n", "comm.Gatherv(vpha_sub, [vpha, (sendcounts, None), ltype], root=0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Make Sure Everyone Has Reported Back Before Moving On"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["comm.Barrier()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Free Data Type"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["cntype.Free()\n", "ltype.Free()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Re-organize Solutions"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if (rank == 0):\n", "    narr,nidx = np.unique(sta_idx,return_index=True)\n", "    try:\n", "        eamp = eamp[nidx,:]; namp = namp[nidx,:]; vamp = vamp[nidx,:]\n", "        epha = epha[nidx,:]; npha = npha[nidx,:]; vpha = vpha[nidx,:]\n", "    except:\n", "        eamp = eamp[nidx]; namp = namp[nidx]; vamp = vamp[nidx]\n", "        epha = epha[nidx]; npha = npha[nidx]; vpha = vpha[nidx]\n", "    #print('Up amplitude (rows = stations; cols = load cells):')\n", "    #print(vamp)\n", "    #print('Up phase (rows = stations; cols = load cells):')\n", "    #print(vpha)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Loop Through Each Station & Populate the Design Matrix"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for jj in range(0,len(slat)):\n", "    if (rank == 0):\n\n", "        # Remove Index If Only 1 Station\n", "        if (numel == 1): # only 1 station read in\n", "            csta = sta\n", "            clat = slat\n", "            clon = slon\n", "        else:\n", "            csta = sta[jj]\n", "            clat = slat[jj]\n", "            clon = slon[jj]\n\n", "        # Convert Amp/Pha to Displacement\n", "        edisp = np.multiply(eamp[jj,:],np.cos(np.multiply(epha[jj,:],(np.pi/180.))))\n", "        ndisp = np.multiply(namp[jj,:],np.cos(np.multiply(npha[jj,:],(np.pi/180.))))\n", "        udisp = np.multiply(vamp[jj,:],np.cos(np.multiply(vpha[jj,:],(np.pi/180.))))\n\n", "        # Fill in Design Matrix\n", "        idxe = (jj*3)+0\n", "        idxn = (jj*3)+1\n", "        idxu = (jj*3)+2\n", "        desmat[idxe,:] = edisp\n", "        desmat[idxn,:] = ndisp\n", "        desmat[idxu,:] = udisp\n", "        dmrows[idxe] = (csta + 'E')\n", "        dmrows[idxn] = (csta + 'N')\n", "        dmrows[idxu] = (csta + 'U')\n", "        sclat[idxe] = clat\n", "        sclat[idxn] = clat\n", "        sclat[idxu] = clat\n", "        sclon[idxe] = clon\n", "        sclon[idxn] = clon\n", "        sclon[idxu] = clon"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Write Design Matrix to File"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if (rank == 0):\n", "    print(\":: Writing netCDF-formatted file.\")\n", "    f_out = (\"designmatrix_\" + rfm + \"_\" + outstr + \".nc\")\n", "    f_file = (\"../output/DesignMatrixLoad/\" + f_out)\n", "    # Open new NetCDF file in \"write\" mode\n", "    dataset = netCDF4.Dataset(f_file,'w',format='NETCDF4_CLASSIC')\n", "    # Define dimensions for variables\n", "    desmat_shape = desmat.shape\n", "    num_rows = desmat_shape[0]\n", "    num_cols = desmat_shape[1]\n", "    nstacomp = dataset.createDimension('nstacomp',num_rows)\n", "    nloadcell = dataset.createDimension('nloadcell',num_cols)\n", "    nchars = dataset.createDimension('nchars',10)\n", "    # Create variables\n", "    sta_comp_id = dataset.createVariable('sta_comp_id','S1',('nstacomp','nchars'))\n", "    load_cell_id = dataset.createVariable('load_cell_id','S1',('nloadcell','nchars'))\n", "    design_matrix = dataset.createVariable('design_matrix',float,('nstacomp','nloadcell'))\n", "    sta_comp_lat = dataset.createVariable('sta_comp_lat',float,('nstacomp',))\n", "    sta_comp_lon = dataset.createVariable('sta_comp_lon',float,('nstacomp',))\n", "    load_cell_lat = dataset.createVariable('load_cell_lat',float,('nloadcell',))\n", "    load_cell_lon = dataset.createVariable('load_cell_lon',float,('nloadcell',))\n", "    # Add units\n", "    sta_comp_id.units = 'string'\n", "    sta_comp_id.long_name = 'station_component_id'\n", "    load_cell_id.units = 'string'\n", "    load_cell_id.long_name = 'load_cell_id'\n", "    design_matrix.units = 'mm'\n", "    design_matrix.long_name = 'displacement_mm'\n", "    sta_comp_lat.units = 'degrees_north'\n", "    sta_comp_lat.long_name = 'station_latitude'\n", "    sta_comp_lon.units = 'degrees_east'\n", "    sta_comp_lon.long_name = 'station_longitude'\n", "    load_cell_lat.units = 'degrees_north'\n", "    load_cell_lat.long_name = 'loadcell_latitude'\n", "    load_cell_lon.units = 'degrees_east'\n", "    load_cell_lon.long_name = 'loadcell_longitude'\n", "    # Assign data\n", "    #  https://unidata.github.io/netcdf4-python/ (see \"Dealing with Strings\")\n", "    #  sta_comp_id[:] = netCDF4.stringtochar(np.array(dmrows,dtype='S10'))\n", "    #  load_cell_id[:] = netCDF4.stringtochar(np.array(load_cells,dtype='S10'))\n", "    sta_comp_id._Encoding = 'ascii'\n", "    sta_comp_id[:] = np.array(dmrows,dtype='S10')\n", "    load_cell_id._Encoding = 'ascii'\n", "    load_cell_id[:] = np.array(load_cells,dtype='S10')\n", "    design_matrix[:,:] = desmat\n", "    sta_comp_lat[:] = sclat\n", "    sta_comp_lon[:] = sclon\n", "    load_cell_lat[:] = lclat\n", "    load_cell_lon[:] = lclon\n", "    # Write Data to File\n", "    dataset.close()\n\n", "    # Read the netCDF file as a test\n", "    f = netCDF4.Dataset(f_file)\n", "    #print(f.variables)\n", "    sta_comp_ids = f.variables['sta_comp_id'][:]\n", "    load_cell_ids = f.variables['load_cell_id'][:]\n", "    design_matrix = f.variables['design_matrix'][:]\n", "    sta_comp_lat = f.variables['sta_comp_lat'][:]\n", "    sta_comp_lon = f.variables['sta_comp_lon'][:]\n", "    load_cell_lat = f.variables['load_cell_lat'][:]\n", "    load_cell_lon = f.variables['load_cell_lon'][:]\n", "    f.close()\n", "    print(sta_comp_ids)\n", "    print(design_matrix)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["--------------------- END CODE --------------------------- #"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}