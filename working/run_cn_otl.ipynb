{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#!/usr/bin/env python"]}, {"cell_type": "markdown", "metadata": {}, "source": ["*********************************************************************<br>\n", "MAIN PROGRAM TO PREDICT SURFACE DISPLACEMENTS CAUSED BY SURFACE MASS LOADING <br>\n", "BY CONVOLVING DISPLACEMENT LOAD GREENS FUNCTIONS WITH A MODEL FOR A SURFACE MASS LOAD <br>\n", "<br>\n", "Copyright (c) 2014-2024: HILARY R. MARTENS, LUIS RIVERA, MARK SIMONS         <br>\n", "<br>\n", "This file is part of LoadDef.<br>\n", "<br>\n", "   LoadDef is free software: you can redistribute it and/or modify<br>\n", "   it under the terms of the GNU General Public License as published by<br>\n", "   the Free Software Foundation, either version 3 of the License, or<br>\n", "   any later version.<br>\n", "<br>\n", "   LoadDef is distributed in the hope that it will be useful,<br>\n", "   but WITHOUT ANY WARRANTY; without even the implied warranty of<br>\n", "   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the<br>\n", "   GNU General Public License for more details.<br>\n", "<br>\n", "   You should have received a copy of the GNU General Public License<br>\n", "   along with LoadDef.  If not, see <https://www.gnu.org/licenses/>.<br>\n", "<br>\n", "*********************************************************************"]}, {"cell_type": "markdown", "metadata": {}, "source": ["IMPORT PRINT FUNCTION"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from __future__ import print_function"]}, {"cell_type": "markdown", "metadata": {}, "source": ["IMPORT MPI MODULE"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from mpi4py import MPI"]}, {"cell_type": "markdown", "metadata": {}, "source": ["MODIFY PYTHON PATH TO INCLUDE 'LoadDef' DIRECTORY"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import sys\n", "import os\n", "sys.path.append(os.getcwd() + \"/../\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["IMPORT PYTHON MODULES"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import scipy as sc\n", "import datetime\n", "import netCDF4\n", "from math import pi\n", "from CONVGF.utility import read_station_file\n", "from CONVGF.utility import read_lsmask\n", "from CONVGF.utility import read_greens_fcn_file\n", "from CONVGF.utility import read_greens_fcn_file_norm\n", "from CONVGF.utility import normalize_greens_fcns\n", "from CONVGF.utility import read_AmpPha\n", "from CONVGF.CN import load_convolution\n", "from CONVGF.CN import interpolate_load\n", "from CONVGF.CN import compute_specific_greens_fcns\n", "from CONVGF.CN import generate_integration_mesh\n", "from CONVGF.CN import intmesh2geogcoords\n", "from CONVGF.CN import integrate_greens_fcns\n", "from CONVGF.CN import compute_angularDist_azimuth\n", "from CONVGF.CN import interpolate_lsmask\n", "from CONVGF.CN import coef2amppha\n", "from CONVGF.CN import mass_conservation"]}, {"cell_type": "markdown", "metadata": {}, "source": ["--------------- SPECIFY USER INPUTS --------------------- #"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Reference Frame (used for filenames) [Blewitt 2003]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["rfm = \"cm\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Greens Function File<br>\n", " :: May be load Green's function file output directly from run_gf.py (norm_flag = False)<br>\n", " :: May be from a published table, normalized according to Farrell (1972) conventions [theta, u_norm, v_norm]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["pmod = \"PREM\"\n", "grn_file = (\"../output/Greens_Functions/\" + rfm + \"_\" + pmod + \".txt\")\n", "norm_flag  = False"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Full Path to Load Directory and Prefix of Filename"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["loadfile_directory = (\"../output/Grid_Files/nc/OTL/\")  # Example 1 (ocean tidal loading)\n", "#loadfile_directory = (\"../output/Grid_Files/nc/NTOL/\")  # Example 2 (time series)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Prefix for the Load Files (Load Directory will be Searched for all Files Starting with this Prefix)<br>\n", " :: Note: For Load Files Organized by Date, the End of Filename Name Must be in the Format yyyymmddhhmnsc.txt<br>\n", " :: Note: If not organized by date, files may be organized by tidal harmonic, for example (i.e. a unique filename ending)<br>\n", " :: Note: Output names (within output files) will be determined by extension following last underscore character (e.g., date/harmonic/model)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["loadfile_prefix = (\"convgf_GOT410c\") # Example 1 (ocean tidal loading)\n", "#loadfile_prefix = (\"convgf_ntol\") # Example 2 (time series)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["LoadFile Format: [\"nc\", \"txt\"]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["loadfile_format = \"nc\"\n", " \n", "# Are the Load Files Organized by Datetime?\n", "#  :: If False, all Files that match the loadfile directory and prefix will be analyzed.\n", "time_series = False  # Example 1 (ocean tidal loading)\n", "#time_series = True  # Example 2 (time series)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Date Range for Computation (Year,Month,Day,Hour,Minute,Second)<br>\n", " :: Note: Only used if 'time_series' is True"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["frst_date = [2015,1,1,0,0,0]\n", "last_date = [2016,3,1,0,0,0]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Are the load values on regular grids (speeds up interpolation); If unsure, leave as false."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["regular = True"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Load Density<br>\n", " Recommended: 1025-1035 kg/m^3 for oceanic loads (e.g., FES2014, ECCO2); 1 kg/m^3 for atmospheric loads (e.g. ECMWF); 1000 kg/m^3 for fresh water"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ldens = 1030.0"]}, {"cell_type": "markdown", "metadata": {}, "source": ["NEW OPTION: Provide a common geographic mesh?<br>\n", "If True, must provide the full path to a mesh file (see: GRDGEN/common_mesh). <br>\n", "If False, a station-centered grid will be created within the functions called here. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["common_mesh = False\n", "# Full Path to Grid File Containing Surface Mesh (for sampling the load Green's functions)\n", "#  :: Format: latitude midpoints [float,degrees N], longitude midpoints [float,degrees E], unit area of each patch [float,dimensionless (need to multiply by r^2)]\n", "meshfname = (\"commonMesh_global_1.0_1.0_18.0_60.0_213.0_278.0_0.1_0.1_28.0_50.0_233.0_258.0_0.01_0.01_landmask\")\n", "convmesh = (\"../output/Grid_Files/nc/commonMesh/\" + meshfname + \".nc\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Planet Radius (in meters; used for Greens function normalization)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["planet_radius = 6371000.\n", "  \n", "# Ocean/Land Mask \n", "#  :: 0 = do not mask ocean or land (retain full model); 1 = mask out land (retain ocean); 2 = mask out oceans (retain land)\n", "#  :: Recommended: 1 for oceanic; 2 for atmospheric\n", "lsmask_type = 1"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Full Path to Land-Sea Mask File (May be Irregular and Sparse)<br>\n", " :: Format: Lat, Lon, Mask [0=ocean; 1=land]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["lsmask_file = (\"../input/Land_Sea/ETOPO1_Ice_g_gmt4_wADD.txt\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Enforce mass conservation by removing a spatial mean from the load grid?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["mass_cons = False"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Station/Grid-Point Location File (Lat, Lon, StationName)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sta_file = (\"../input/Station_Locations/NOTA_Select.txt\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Optional: Additional string to include in output filenames (e.g. \"_2019\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if (common_mesh == True):\n", "    mtag = \"commonMesh\"\n", "else:\n", "    mtag = \"stationMesh\"\n", "outstr = (\"_\" + mtag + \"_\" + pmod)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["------------------ END USER INPUTS ----------------------- #"]}, {"cell_type": "markdown", "metadata": {}, "source": ["-------------------- SETUP MPI --------------------------- #"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Get the Main MPI Communicator That Controls Communication Between Processors"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["comm = MPI.COMM_WORLD\n", "# Get My \"Rank\", i.e. the Processor Number Assigned to Me\n", "rank = comm.Get_rank()\n", "# Get the Total Number of Other Processors Used\n", "size = comm.Get_size()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---------------------------------------------------------- #"]}, {"cell_type": "markdown", "metadata": {}, "source": ["-------------------- BEGIN CODE -------------------------- #"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Ensure that the Output Directories Exist & Read in the Stations"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if (rank == 0):\n", "    if not (os.path.isdir(\"../output/Convolution/\")):\n", "        os.makedirs(\"../output/Convolution/\")\n", "    if not (os.path.isdir(\"../output/Convolution/temp/\")):\n", "        os.makedirs(\"../output/Convolution/temp/\")\n", "    tempdir = \"../output/Convolution/temp/\"\n\n", "    # Read Station File\n", "    slat,slon,sta = read_station_file.main(sta_file)\n", " \n", "    # Ensure that Station Locations are in Range 0-360\n", "    neglon_idx = np.where(slon<0.)\n", "    slon[neglon_idx] += 360.\n\n", "    # Determine Number of Stations Read In\n", "    if isinstance(slat,float) == True: # only 1 station\n", "        numel = 1\n", "    else:\n", "        numel = len(slat)\n", " \n", "    # Generate an Array of File Indices\n", "    sta_idx = np.linspace(0,numel,num=numel,endpoint=False)\n", "    np.random.shuffle(sta_idx)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["else: # If I'm a worker, I know nothing yet about the data\n", "    slat = slon = sta = numel = sta_idx = None"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Make Sure Everyone Has Reported Back Before Moving On"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["comm.Barrier()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["All Processors Get Certain Arrays and Parameters; Broadcast Them"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sta          = comm.bcast(sta, root=0)\n", "slat         = comm.bcast(slat, root=0)\n", "slon         = comm.bcast(slon, root=0)\n", "numel        = comm.bcast(numel, root=0)\n", "sta_idx      = comm.bcast(sta_idx, root=0)\n", " \n", "# MPI: Determine the Chunk Sizes for the Convolution\n", "total_stations = len(slat)\n", "nominal_load = total_stations // size # Floor Divide\n", "# Final Chunk Might Be Different in Size Than the Nominal Load\n", "if rank == size - 1:\n", "    procN = total_stations - rank * nominal_load\n", "else:\n", "    procN = nominal_load"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Make some preparations that are common to all stations"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if (rank == 0):  \n\n", "    # Read in the Land-Sea Mask\n", "    if (lsmask_type > 0):\n", "        lslat,lslon,lsmask = read_lsmask.main(lsmask_file)\n", "    else:\n", "        # Doesn't really matter so long as there are some values filled in with something other than 1 or 2\n", "        lat1d = np.arange(-90.,90.,2.)\n", "        lon1d = np.arange(0.,360.,2.)\n", "        olon,olat = np.meshgrid(lon1d,lat1d)\n", "        lslat = olat.flatten()\n", "        lslon = olon.flatten()\n", "        lsmask = np.ones((len(lslat),)) * -1.\n\n", "    # Ensure that Land-Sea Mask Longitudes are in Range 0-360\n", "    neglon_idx = np.where(lslon<0.)\n", "    lslon[neglon_idx] += 360. \n", " \n", "    # Convert Start and End Dates to Datetimes\n", "    if (time_series == True):\n", "        frstdt = datetime.datetime(frst_date[0],frst_date[1],frst_date[2],frst_date[3],frst_date[4],frst_date[5])\n", "        lastdt = datetime.datetime(last_date[0],last_date[1],last_date[2],last_date[3],last_date[4],last_date[5])\n\n", "    # Check format of load files\n", "    if not (loadfile_format == \"nc\"):\n", "        if not (loadfile_format == \"txt\"):\n", "            print(\":: Error: Invalid format for load files. See scripts in the /GRDGEN/load_files/ folder. \\\n", "                Acceptable formats: netCDF, txt.\")\n\n", "    # Determine Number of Matching Load Files\n", "    load_files = []\n", "    if os.path.isdir(loadfile_directory):\n", "        for mfile in os.listdir(loadfile_directory): # Filter by Load Directory\n", "            if mfile.startswith(loadfile_prefix): # Filter by File Prefix\n", "                if (time_series == True):\n", "                    if (loadfile_format == \"txt\"):\n", "                        mydt = datetime.datetime.strptime(mfile[-18:-4],'%Y%m%d%H%M%S') # Convert Filename String to Datetime\n", "                    elif (loadfile_format == \"nc\"):\n", "                        mydt = datetime.datetime.strptime(mfile[-17:-3],'%Y%m%d%H%M%S') # Convert Filename String to Datetime\n", "                    else:\n", "                        print(\":: Error: Invalid format for load files. See scripts in the /GRDGEN/load_files/ folder. \\\n", "                            Acceptable formats: netCDF, txt.\")\n", "                    if ((mydt >= frstdt) & (mydt <= lastdt)): # Filter by Date Range\n", "                        load_files.append(loadfile_directory + mfile) # Append File to List\n", "                else:\n", "                    load_files.append(loadfile_directory + mfile) # Append File to List\n", "    else:\n", "        sys.exit('Error: The loadfile directory does not exist. You may need to create it. \\\n", "            The /GRDGEN/load_files/ folder contains utility scripts to convert common models into \\\n", "            LoadDef-compatible formats, and will automatically create a loadfile directory.')\n\n", "    # Test for Load Files\n", "    if not load_files:\n", "        sys.exit('Error: Could not find load files. You may need to generate them. \\\n", "            The /GRDGEN/load_files/ folder contains utility scripts to convert \\\n", "            common models into LoadDef-compatible formats.')\n\n", "    # Sort the Filenames\n", "    load_files = np.asarray(load_files)\n", "    fidx = np.argsort(load_files)\n", "    load_files = load_files[fidx]\n", "    num_lfiles = len(load_files)\n", " \n", "    # Initialize Arrays\n", "    eamp = np.empty((numel,num_lfiles))\n", "    epha = np.empty((numel,num_lfiles))\n", "    namp = np.empty((numel,num_lfiles))\n", "    npha = np.empty((numel,num_lfiles))\n", "    vamp = np.empty((numel,num_lfiles))\n", "    vpha = np.empty((numel,num_lfiles))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["If I'm a Worker, I Know Nothing About the Data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["else:\n", "    lslat = lslon = lsmask = load_files = None\n", "    eamp = epha = namp = npha = vamp = vpha = None"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Make Sure Everyone Has Reported Back Before Moving On"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["comm.Barrier()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Prepare the common mesh, if applicable"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if (rank == 0):\n", "    if (common_mesh == True): \n\n", "        ## Read in the common mesh\n", "        print(':: Common Mesh True. Reading in ilat, ilon, iarea.')\n", "        lcext = convmesh[-2::]\n", "        if (lcext == 'xt'):\n", "            ilat,ilon,unit_area = np.loadtxt(convmesh,usecols=(0,1,2),unpack=True)\n", "            # convert from unit area to true area of the spherical patch in m^2\n", "            iarea = np.multiply(unit_area, planet_radius**2) \n", "        elif (lcext == 'nc'):\n", "            f = netCDF4.Dataset(convmesh)\n", "            ilat = f.variables['midpoint_lat'][:]\n", "            ilon = f.variables['midpoint_lon'][:]\n", "            unit_area = f.variables['unit_area_patch'][:]\n", "            f.close()\n", "            # convert from unit area to true area of the spherical patch in m^2\n", "            iarea = np.multiply(unit_area, planet_radius**2)\n\n", "        ## Determine the Land-Sea Mask: Interpolate onto Mesh\n", "        print(':: Common Mesh True. Applying Land-Sea Mask.')\n", "        print(':: Number of Grid Points: %s | Size of LSMask: %s' %(str(len(ilat)), str(lsmask.shape)))\n", "        lsmk = interpolate_lsmask.main(ilat,ilon,lslat,lslon,lsmask)\n", "        print(':: Finished LSMask Interpolation.')\n\n", "        ## For a common mesh, can already interpolate the load(s) onto the mesh, and also apply the land-sea mask.\n", "        ## Prepare land-sea mask application\n", "        if (lsmask_type == 2): \n", "            test_elements = np.where(lsmk == 0); test_elements = test_elements[0]\n", "        elif (lsmask_type == 1): \n", "            test_elements = np.where(lsmk == 1); test_elements = test_elements[0]\n\n", "        ## Loop through load file(s)\n", "        full_files = []\n", "        for hh in range(0,len(load_files)):\n\n", "            ## Current load file\n", "            cldfile = load_files[hh]\n\n", "            ## Filename identifier\n", "            str_components = cldfile.split('_')\n", "            cext = str_components[-1]\n", "            if (loadfile_format == \"txt\"):\n", "                file_id = cext[0:-4]\n", "            elif (loadfile_format == \"nc\"):\n", "                file_id = cext[0:-3]\n", "            else:\n", "                print(':: Error. Invalid file format for load models. [load_convolution.py]')\n", "                sys.exit()\n\n", "            ## Read the File\n", "            llat,llon,amp,pha,llat1dseq,llon1dseq,amp2darr,pha2darr = read_AmpPha.main(cldfile,loadfile_format,regular_grid=regular)\n", "            ## Find Where Amplitude is NaN (if anywhere) and Set to Zero\n", "            nanidx = np.isnan(amp); amp[nanidx] = 0.; pha[nanidx] = 0.\n", "            ## Convert Amp/Pha Arrays to Real/Imag\n", "            real = np.multiply(amp,np.cos(np.multiply(pha,pi/180.)))\n", "            imag = np.multiply(amp,np.sin(np.multiply(pha,pi/180.)))\n", "            \n", "            ## Interpolate Load at Each Grid Point onto the Integration Mesh\n", "            ic1,ic2   = interpolate_load.main(ilat,ilon,llat,llon,real,imag,regular)\n", "            \n", "            ## Multiply the Load Heights by the Load Density\n", "            ic1 = np.multiply(ic1,ldens)\n", "            ic2 = np.multiply(ic2,ldens)\n", "            \n", "            ## Enforce Mass Conservation, if Desired\n", "            if (mass_cons == True):\n", "                if (lsmask_type == 1): # For Oceans\n", "                    print(':: Warning: Enforcing Mass Conservation Over Oceans.')\n", "                    ic1_mc,ic2_mc = mass_conservation.main(ic1[lsmk==0],ic2[lsmk==0],iarea[lsmk==0])\n", "                    ic1[lsmk==0] = ic1_mc\n", "                    ic2[lsmk==0] = ic2_mc\n", "                else: # For Land and Whole-Globe Models (like atmosphere and continental water)\n", "                    print(':: Warning: Enforcing Mass Conservation Over Entire Globe.')\n", "                    ic1,ic2 = mass_conservation.main(ic1,ic2,iarea)\n", "            \n", "            ## Apply Land-Sea Mask Based on LS Mask Database (LAND=1;OCEAN=0) \n", "            # If lsmask_type = 2, Set Oceans to Zero (retain land)\n", "            # If lsmask_type = 1, Set Land to Zero (retain ocean)\n", "            # Else, Do Nothing (retain full model)\n", "            if (lsmask_type == 2):\n", "                ic1[lsmk == 0] = 0.\n", "                ic2[lsmk == 0] = 0. \n", "            elif (lsmask_type == 1):\n", "                ic1[lsmk == 1] = 0.\n", "                ic2[lsmk == 1] = 0.\n\n", "            ## Write results to temporary netCDF files\n", "            print(\":: Writing netCDF-formatted temporary file for: \", cldfile)\n", "            custom_file = (tempdir + \"temp\" + outstr + \"_\" + file_id + \".nc\")\n", "            full_files.append(custom_file)\n", "            # Open new NetCDF file in \"write\" mode\n", "            dataset = netCDF4.Dataset(custom_file,'w',format='NETCDF4_CLASSIC')\n", "            # Define dimensions for variables\n", "            num_pts = len(ic1)\n", "            latitude = dataset.createDimension('latitude',num_pts)\n", "            longitude = dataset.createDimension('longitude',num_pts)\n", "            real = dataset.createDimension('real',num_pts)\n", "            imag = dataset.createDimension('imag',num_pts)\n", "            parea = dataset.createDimension('area',num_pts)\n", "            # Create variables\n", "            latitudes = dataset.createVariable('latitude',float,('latitude',))\n", "            longitudes = dataset.createVariable('longitude',float,('longitude',))\n", "            reals = dataset.createVariable('real',float,('real',))\n", "            imags = dataset.createVariable('imag',float,('imag',))\n", "            pareas = dataset.createVariable('area',float,('area',))\n", "            # Add units\n", "            latitudes.units = 'degree_north'\n", "            longitudes.units = 'degree_east'\n", "            reals.units = 'kg/m^2 (real part of load * load density)'\n", "            imags.units = 'kg/m^2 (imag part of load * load density)'\n", "            pareas.units = 'm^2 (unit area of patch * planet_radius^2)'\n", "            # Assign data\n", "            latitudes[:] = ilat\n", "            longitudes[:] = ilon\n", "            reals[:] = ic1\n", "            imags[:] = ic2\n", "            pareas[:] = iarea\n", "            # Write Data to File\n", "            dataset.close()\n", "        \n", "        ## Rename file list\n", "        load_files = full_files.copy()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Make Sure Everyone Has Reported Back Before Moving On"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["comm.Barrier()"]}, {"cell_type": "markdown", "metadata": {}, "source": [" If Using a Common Mesh, Then Re-set the LoadFile Format to Indicate a Common Mesh is Used"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if (common_mesh == True): \n", "    loadfile_format = \"common\"\n", "  \n", "# All Processors Get Certain Arrays and Parameters; Broadcast Them\n", "lslat        = comm.bcast(lslat, root=0)\n", "lslon        = comm.bcast(lslon, root=0)\n", "lsmask       = comm.bcast(lsmask, root=0)\n", "load_files   = comm.bcast(load_files, root=0)\n", "eamp         = comm.bcast(eamp, root=0)\n", "epha         = comm.bcast(epha, root=0)\n", "namp         = comm.bcast(namp, root=0)\n", "npha         = comm.bcast(npha, root=0)\n", "vamp         = comm.bcast(vamp, root=0)\n", "vpha         = comm.bcast(vpha, root=0) "]}, {"cell_type": "markdown", "metadata": {}, "source": ["Gather the Processor Workloads for All Processors"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sendcounts = comm.gather(procN, root=0)\n", " \n", "# Create a Data Type for the Convolution Results\n", "cntype = MPI.DOUBLE.Create_contiguous(1)\n", "cntype.Commit()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Create a Data Type for Convolution Results for each Station and Load File"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["num_lfiles = len(load_files)\n", "ltype = MPI.DOUBLE.Create_contiguous(num_lfiles)\n", "ltype.Commit()\n", " \n", "# Scatter the Station Locations (By Index)\n", "d_sub = np.empty((procN,))\n", "comm.Scatterv([sta_idx, (sendcounts, None), cntype], d_sub, root=0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Set up the arrays"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["eamp_sub = np.empty((len(d_sub),num_lfiles))\n", "epha_sub = np.empty((len(d_sub),num_lfiles))\n", "namp_sub = np.empty((len(d_sub),num_lfiles))\n", "npha_sub = np.empty((len(d_sub),num_lfiles))\n", "vamp_sub = np.empty((len(d_sub),num_lfiles))\n", "vpha_sub = np.empty((len(d_sub),num_lfiles))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Loop through the stations"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for ii in range(0,len(d_sub)):\n\n", "    # Current station\n", "    current_sta = int(d_sub[ii]) # Index\n\n", "    # Remove Index If Only 1 Station\n", "    if (numel == 1): # only 1 station read in\n", "        csta = sta\n", "        clat = slat\n", "        clon = slon\n", "    else:\n", "        csta = sta[current_sta]\n", "        clat = slat[current_sta]\n", "        clon = slon[current_sta]\n\n", "    # If Rank is Main, Output Station Name\n", "    try:\n", "        csta = csta.decode()\n", "    except:\n", "        pass\n\n", "    # Output File Name\n", "    cnv_out = csta + \"_\" + rfm + \"_\" + loadfile_prefix + outstr + \".txt\"\n\n", "    # Status update\n", "    print(':: Working on station: %s | Number: %6d of %6d | Rank: %6d' %(csta, (ii+1), len(d_sub), rank))\n", "    \n", "    # Compute Convolution for Current File\n", "    eamp_sub[ii,:],epha_sub[ii,:],namp_sub[ii,:],npha_sub[ii,:],vamp_sub[ii,:],vpha_sub[ii,:] = load_convolution.main(\\\n", "        grn_file,norm_flag,load_files,loadfile_format,regular,lslat,lslon,lsmask,lsmask_type,clat,clon,csta,cnv_out,load_density=ldens)\n", "  \n", "# Gather Results\n", "comm.Gatherv(eamp_sub, [eamp, (sendcounts, None), ltype], root=0)\n", "comm.Gatherv(epha_sub, [epha, (sendcounts, None), ltype], root=0)\n", "comm.Gatherv(namp_sub, [namp, (sendcounts, None), ltype], root=0)\n", "comm.Gatherv(npha_sub, [npha, (sendcounts, None), ltype], root=0)\n", "comm.Gatherv(vamp_sub, [vamp, (sendcounts, None), ltype], root=0)\n", "comm.Gatherv(vpha_sub, [vpha, (sendcounts, None), ltype], root=0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Make Sure Everyone Has Reported Back Before Moving On"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["comm.Barrier()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Free Data Type"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["cntype.Free()\n", "ltype.Free()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Re-organize Solutions"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if (rank == 0):\n", "    narr,nidx = np.unique(sta_idx,return_index=True)\n", "    try:\n", "        eamp = eamp[nidx,:]; namp = namp[nidx,:]; vamp = vamp[nidx,:]\n", "        epha = epha[nidx,:]; npha = npha[nidx,:]; vpha = vpha[nidx,:]\n", "    except: \n", "        eamp = eamp[nidx]; namp = namp[nidx]; vamp = vamp[nidx]\n", "        epha = epha[nidx]; npha = npha[nidx]; vpha = vpha[nidx]\n", "    #print('Up amplitude (rows = stations; cols = load models):')\n", "    #print(vamp)\n", "    #print('Up phase (rows = stations; cols = load models):')\n", "    #print(vpha)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Make Sure All Jobs Have Finished Before Continuing"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["comm.Barrier()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Remove load files that are no longer needed"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if (rank == 0):\n", "    if (common_mesh == True): \n", "        for gg in range(0,len(load_files)): \n", "            cfile = load_files[gg]\n", "            os.remove(cfile) "]}, {"cell_type": "markdown", "metadata": {}, "source": ["Make Sure All Jobs Have Finished Before Continuing"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["comm.Barrier()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["--------------------- END CODE --------------------------- #"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}